---
title: "Introduction to spatial microsimulation with R"
author: "Robin Lovelace"
date: "May 2015"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: latex/cdrc-template.tex
    number_sections: yes
    toc: yes
  word_document: default
bibliography: smr.bib
---

# Introduction

These notes accompany a 1.5 day short course, *Introduction to Spatial Microsimulation using R*. The notes have evolved over time to focus on
tools to automate the creation of synthetic spatial microdata.
Early versions taught methods to hard-code
the reweighting algorithms in R.^[See https://github.com/Robinlovelace/smsim-course/blob/master/handout.pdf for a version of the course taught in 2014.]
You'll be thankful to know that
functions have been developed to make life easier since then!

These notes (and the underlying code that was used to write them)
are available in their entirety from github.com/Robinlovelace/. 

The other important development is that early notes have been
expanded to create a book,
entitled *Spatial Microsimulation with R* [@lovelace-dumont2015].
These notes are a subset of this book. For a more detailed account
of spatial microsimulation with R readers are directed to the
book's homepage:
[robinlovelace.net/spatial-microsim-book](http://robinlovelace.net/spatial-microsim-book/).

Any feedback is greatly appreciated, via the social coding website github or via email --- you can reach me at `r.lovelace@leeds.ac.uk`.

# Data preparation {#data-prep}

```{r, echo=FALSE}
library(png)
library(grid)
```

The first stage of most data-intensive projects is data preparation and
spatial microsimulation is no exception.

This section describes the input datasets needed for spatial
microsimulation. The
section is practical, so it will get you up-to-speed with
R via RStudio. The aim is twofold: 1) to produce
data that is ready for population synthesis and 2) to acquaint
demonstrate the software and a workflow
that will make developing spatial microsimulation models as easy as possible.

Before loading the spatial microdata, one must decide on the data 
needed for your research. Focus on "target variables" related to the research
question will help decide on the constraint variables, as we will see
below.
Selecting only on variables of interest will ensure you do not
waste time thinking about and processing 
additional variables that you will be of little use in the future.

There are two input datasets:
1) a non-geographical individual-level dataset
(the microdata) and
2) geographical count data, in which each row is a geographical zone.

The data used in this Chapter and throughout the book can be
downloaded from the book's [GitHub repository](https://github.com/Robinlovelace/smsim-course), located at
https://github.com/Robinlovelace/smsim-course

From this page, click on the
'Download ZIP' button to the right and extract the folder into a
sensible place on your computer, such as the Desktop.  From there, 
run R from the project's root directory. To do this
open the newly created folder in a file browser
and double click on `spatial-microsim-book.Rproj` (Fig. 1).  This should 
launch RStudio 
at this location, with all the input data files easily accessible to R
through *relative file paths*.

```{r studio,  fig.cap="The RStudio interface with the 'msim-course' project loaded", echo=FALSE, message=FALSE, fig.height=3}
# todo: create tutorial on installing R and RStudio
grid.raster(readPNG("figures/rstudio.png"))
```

To ease reproducibility of the analysis when working with real data, it is
recommended that the process begins with a copy of the *raw* dataset on one's
hard disc. Rather than modifying this file, modified ('cleaned') versions
should be saved as separate files. 
We use a relatively clean and
very tiny input dataset from SimpleWorld, but we still create a backup of the
original. We will see in the next section
how to deal with larger and messier data.

The process of loading, checking and preparing the input datasets for spatial
microsimulation is generally a linear process, encapsulating the following
stages:

1. Selecting the constraint and target variables
2. Loading the data 
3. Re-categorise individual-level data
4. Set variable and value names
5. 'Flatten' individual-level data 

## Selecting target and constraint variables {#Selecting}

The first consideration for the constraint data
is coverage: ideally close to 100% of each zone's total
population should be included in the counts. Also, the
survey must have been completed by a number of residents  proportional to the
total number of inhabitants in every geographical zone under consideration. 

The following considerations should inform the selection of
individual-level microdata:

- Linking variables: are there enough
variables in the individual-level data that are
also present in the geographically aggregated constraints?
- Representiveness: is the population sample in the survey microdata
representative of the areas under investigation?
- Sample size and diversity: the input dataset must be sufficiently large and diverse to mitigate the *empty cell* problem [@Lovelace-ipfinr].
- Target variables: are these, or proxies for them, included in the dataset?

There is no 'magic number' that is correct in every case.
It is also important to note that the number of variables
is not a particularly good measure of how well constrained
the data will be. The total number of *categories* used to
constrain the weights is in fact more important.

## Loading input data {#Loading} 

Real-world individual-level data are provided in many formats.
These ultimately need to be loaded into R as a `data.frame` object. Note 
that there are many ways to load data into R, including `read.csv()` and
`read.table()` from base R. Useful commands for loading proprietary data formats
include `read_excel` and  `read.spss()` from **readxl** and **foreign**
packages respectively.

More time consuming is cleaning the data and there are also many ways to do this.
In the following example we present 
steps needed to load the data underlying the SimpleWorld
example into R.

In the SimpleWorld example, the individual-level dataset is loaded
from a 'plain text' (human readable) `.csv` file:

```{r}
# Load the individual-level data
ind <- read.csv("data/SimpleWorld/ind-full.csv") 
class(ind) # verify the data type of the object
ind # print the individual-level data
```

Constraint data are usually made available one variable at a time,
so these are read in one file at a time:

```{r}
con_age <- read.csv("data/SimpleWorld/age.csv")
con_sex <- read.csv("data/SimpleWorld/sex.csv")
```

We have loaded the aggregate constraints. As with the individual level data, it is
worth inspecting each object to ensure that they make sense before continuing.
Taking a look at `age_con`, we can see that this data set consists of 2
categories for 3 zones:

```{r}
con_age
```

This tells us that there 12, 10 and 11 individuals in zones 1, 2 and 3,
respectively, with different proportions of young and old people. Zone 2, for
example, is heavily dominated by older people: there are 8 people over 50 whilst
there are only 2 young people (under 49) in the zone.

Even at this stage there is a potential for errors to be introduced.  A classic
mistake with areal (geographically aggregated) data is that the order in which zones are loaded can change from
one table to the next.

```{r, echo=FALSE}
# Make the constraint data contain an 'id' column, possibly scrambled 
```

The next important test is to check the total
populations of the constraint variables.  Ideally both the *total*
study area populations and *row totals* should match. If the *row totals* match,
this is a very good sign that not only confirms that the zones are listed in the
same order, but also that each variable is sampling from the same *population
base*. These tests are conducted in the following lines of code:

```{r}
sum(con_age)
sum(con_sex) 

rowSums(con_age)
rowSums(con_sex)
rowSums(con_age) == rowSums(con_sex)
```

The results of the previous operations are encouraging. The total population is
the same for each constraint overall and for each area (row) for both
constraints.  If the total populations between constraint variables do not match
(e.g. because the *population bases* are different),
the errant constraint variables must be scaled.

## Subsetting to remove excess information {#subsetting-prep}

In the above code, `data.frame` objects containing precisely the information
required for the next stage were loaded.  More often, superfluous information
will need to be removed from the data and subsets taken. 
Note that
only the relevant variables, corresponding to the first, third and nineteenth
columns are
retained:

```{r, eval=FALSE}
ind <- ind[, c(1, 3, 19)]
```

In the SimpleWorld dataset, only the `age` and `sex` variables are useful
for reweighting: we can remove the others for the purposes of allocating
individuals to zone.
Before removing the superflous `income` variable, we will create a backup of
`ind` that can be referred back-to when we have a spatial microdataset:

```{r}
ind_orig <- ind # store the original ind dataset for future reference
ind <- ind[, -4] # remove income variable
```

## Re-categorising individual-level variables {#re-categorise}

Before transforming the individual-level dataset `ind` into a form that can be
compared with the aggregate-level constraints, we must ensure that each dataset
contains the same information. It can be more challenging to re-categorise
individual-level variables than to re-name or combine aggregate-level variables,
so the former should usually be set first.  An obvious difference between the
individual and aggregate versions of the `age` variable is that the former is of
type `integer` whereas the latter is composed of discrete bins: 0 to 49 and 50+.
We can categories the variable into these bins using 
`cut()`:

```{r}
# Test binning the age variable
brks <- c(0, 49, 120) # set break points from 0 to 120 years
cut(ind$age, breaks = brks) # bin the age variable
```

Note that the output of the above `cut()` command is correct,
with individuals binned into one of two bins, but that the labels
are rather
strange.
To change these category labels to something more readable
for people who do not read ISO standards for mathematical notation
(most people!),
we can add another argument, `labels` to the `cut()` function:

```{r}
# Convert age into a categorical variable
labs <- c("a0_49", "a50+") # create the labels
cut(ind$age, breaks = brks, labels = labs)
```

The factor generated now has satisfactory labels: they match the column headings
of the age constraint, so we will save the result. (Note, we are not
loosing any information at this stage because we have saved the orginal
`ind` object as `ind_orig` for future reference.)

```{r}
# Overwrite the age variable with categorical age bands
ind$age <- cut(ind$age, breaks = brks, labels = labs)
```

## Matching individual and aggregate level data names {#matching}

Before combining the newly recategorised individual-level data with the
aggregate constraints, it is useful to for the category labels to match up.
This may seem trivial, but will save time in the long run. Here is the problem:

```{r}
levels(ind$age)
names(con_age)
```

Note that the names are subtly different. To solve this issue, we can
simply change the names of the constraint variable, after verifying they
are in the correct order:

```{r}
names(con_age) <- levels(ind$age) # rename aggregate variables
```

With both the age and sex constraint variable names now matching the
category labels of the individual-level data, we can proceed to create a
single constraint object we label `cons`. We do this with `cbind()`:

```{r}
cons <- cbind(con_age, con_sex)
cons[1:2, ] # display the constraints for the first two zones
```

## 'Flattening' the individual level data {#flattening}

We have made steps towards combining the individual and aggregate datasets and
now only need to deal with 2 objects (`ind` and `cons`) which now share
category and variable names.
However, these datasets cannot possibly be compared because they
measure very different things. The `ind` dataset records
the value that each individual takes for a range of variables, whereas
`cons` counts the number of individuals in different groups at
the geographical level. These datesets are have different dimensions:

```{r, echo=FALSE}
# (Dumont, 2014) I don't like the previous sentence, because even if it is the same dimension
# the real problem is that it does not represent the same thing. So, the dimension is not
# the real reason of it
# Think I've corrected this issue! (RL)
```

```{r}
dim(ind)
dim(cons)
```

The above code confirms this: we have one individual-level dataset comprising 5
individuals with 3 variables (2 of which are constraint variables and the other an ID) and one
aggregate-level constraint table called `cons`, representing 3 zones
with count data for 4 categories across 2 variables.

To undertake this 'flattening' process the
`model.matrix()` function is used to expand each variable in turn.
The result for each variable is a new matrix with the same number of columns as
there are categories in the variable. Note that the order of columns is usually
alphabetical: this can cause problems if the columns in the constraint tables
are not ordered in this way.  
Knoblauch and Maloney (2012) provide a lengthier description of this
flattening process.

The second stage is to use the `colSums()` function 
to take the sum of each column.^[As we shall see in Section \ref{ipfp},
only the former of these is needed if we use the
**ipfp** package for re-weighting the data, but both are presented to enable
a better understanding of how IPF works.]

```{r}
cat_age <- model.matrix(~ ind$age - 1)
cat_sex <- model.matrix(~ ind$sex - 1)[, c(2, 1)]

 # Combine age and sex category columns into single data frame
(ind_cat <- cbind(cat_age, cat_sex)) # brackets -> print result
```

Note that second call to `model.matrix` is suffixed with `[, c(2, 1)]`.
This is to swap the order of the columns: the column variables are produced
from `model.matrix` is alphabetic, whereas the order in which the variables
have been saved in the constraints object `cons` is `male` then `female`.
Let's count the number of individuals
represented in the new `ind_cat` variable, using `colSums`:

```{r}
colSums(ind_cat) # view the aggregated version of ind
ind_agg <- colSums(ind_cat) # save the result
```

The sum of both age and sex variables is 5 
(the total number of individuals): it worked! 

The purpose of the *reweighting* procedure in spatial microsimulation is
to minimise this difference (as measured in TAE above)
by adding high weights to the most representative individuals.

# Population synthesis {#smsimr}

How representative each individual is of each zone is represented by their
*weight* for that zone. Each weight links and individual to a zone.
The number
of weights is therefore equal to number of zones multiplied
by the number of individuals
in the microdata, that is the number of rows in individual-level and constraint tables
respectively.
In terms of the SimpleWorld data loaded in the previous section we have, in
R syntax, `nrow(cons)` zones and `nrow(ind)` individuals.
(Typing those commands with the data loaded should confirm that there are
3 zones and 5 individuals in the input data for the SimpleWorld example).
This means that `nrow(cons) * nrow(ind)` weights will be estimated
(that is $3 * 5 = 15$ in SimpleWorld). The weights must begin with an initial value:

```{r}
# Create the weight matrix. Note: relies on data from previous chapter.
weights <- matrix(data = 1, nrow = nrow(ind), ncol = nrow(cons))
dim(weights) # dimension of weight matrix: 5 rows by 3 columns
```

The weigth matrix links individual-level data to aggregate-level data.
A weight matrix value of 0 in cell `[i,j]`, for example, suggests that
nobody with the characteristics of individual `i` is present in
zone `j`.
During the IPF procedure these weights are iteratively updated until
they *converge* towards a single result: the final weights which create
a representative population for each zone.

Integerisation, expansion and compression procedures allow fractional
weighting and combinatorial optimisation approaches to population synthesis
to be seen as essentially the same thing (Fig. x).
This equivalence between different methods of population synthesis is
the reason we have labelled this section
*weighting algorithms*: combinatorial optimisation approaches to population synthesis can
be seen as a special case of fractional weighting and vice versa.

```{r, fig.cap="Schematic of different approaches for the creation of spatial microdata encapsulating stochastic combinatorial optimisation and deterministic reweighting algorithms such as IPF. Note that integerisation and 'compression' steps make the results of the two approaches interchangeable, hence our use of the term 'reweighting algorithm' to cover all methods for generating spatial microdata.", echo=FALSE}
img <- readPNG("figures/co-vs-ipf-schema.png")
grid.raster(img)
```

## Reweighting with **ipfp** {#ipfp}

IPF runs much faster and with less code than when hard-coded in R using the
**ipfp** package than in pure R. The `ipfp` function runs the IPF algorithm
in the C language, taking aggregate constraints, individual level
data and an initial weight vector (`x0`) as inputs:

```{r}
library(ipfp) # load ipfp library after install.packages("ipfp")
cons <- apply(cons, 2, as.numeric) # to 1d numeric data type
ipfp(cons[1,], t(ind_cat), x0 = rep(1, nrow(ind))) # run IPF
```

It is impressive that the entire IPF process, which takes dozens of lines of
code in pure R can been condensed into two lines: one to
convert the input constraint dataset to `numeric`
and one to perform the IPF operation itself. 
Let's create some parameters that will be used throughout this section.

```{r}
# Create intuitiv names for the totals
n_zone <- nrow(cons) # number of zones
n_ind <- nrow(ind) # number of individuals
n_age <-ncol(con_age) # number of categories of "age"
n_sex <-ncol(con_sex) # number of categories of "sex"
```

Notice also that for the function to work
a *transposed* (via the `t()` function) version of the individual-level
data (`ind_cat`) was used. This differs from the the
`ind_agg` object used in the pure R version. To prevent having to transpose
`ind_cat` every time `ipfp` is called, we save the transposed version:

```{r}
ind_catt <- t(ind_cat) # save transposed version of ind_cat
```

Another object that can be saved prior to running `ipfp` on all zones
(the rows of `cons`) is `rep(1, nrow(ind))`, simply a series of ones - one for each individual.
We will call this object `x0` as its argument name representing
the starting point of the weight estimates in `ipfp`:

```{r}
x0 <- rep(1, nrow(ind)) # save the initial vector
```

To extend this process to all three zones we can wrap the line beginning
`ipfp(...)` inside a `for` loop, saving the results each time into a
weight variable we created earlier:

```{r}
weights_maxit_2 <- weights # create a copy of the weights object
ncw <- ncol(weights)
for(i in 1:ncw){
  weights_maxit_2[,i] <- ipfp(cons[i,], ind_catt, x0, maxit = 2)
}
```

The above code uses `i` to iterate through the constraints, one row (zone) at
a time, saving the output vector of weights for the individuals into columns
of the weight matrix. To make this process even more concise (albeit
less clear to R beginners), we can use R's internal
`for` loop, `apply`:

```{r}
weights <- apply(cons, MARGIN = 1, FUN = 
    function(x) ipfp(x, ind_catt, x0, maxit = 20))
```

In the above code R iterates through each row
(hence the second argument `MARGIN` being `1`, `MARGIN = 2`
would signify column-wise iteration).
Thus `ipfp` is applied to each zone in turn, as with the `for` loop implementation. 

```{r, echo=FALSE, eval=FALSE}
# Also discuss what happens when you get a huge dataset, from Stephen's dataset
```

It is important to check that the weights obtained from IPF make sense.
To do this, we multiply the weights of each individual by rows of
the `ind_cat` matrix, for each zone. Again, this can be done using
a for loop, but the apply method is more concise:

```{r}
ind_agg <- t(apply(weights, 2, function(x) colSums(x * ind_cat)))
colnames(ind_agg) <- colnames(cons) # make the column names equal
```

As a preliminary test of fit,
it makes sense to check a sample of the aggregated weighted data
(`ind_agg`) against the same sample of the constraints.
Let's look at the results (one would use a subset of the results, 
e.g. `ind_agg[1:3, 1:5]` for the first five values of the first 3
zones for larger constraint tables found in the real world):

```{r}
ind_agg
cons
```

This is a good result: the constraints perfectly match the results
generated using ipf, at least for the sample. To check that this
is due to the `ipfp` algorithm improving the weights with each iteration,
let us analyse the aggregate results generated from the alternative
set of weights, generated with only 2 iterations of IPF:

```{r}
# Update ind_agg values, keeping col names (note '[]')
ind_agg[] <- t(apply(weights_maxit_2, MARGIN = 2, 
  FUN = function(x) colSums(x * ind_cat)))
ind_agg[1:2, 1:4]
```

Clearly the final weights after 2 iterations of IPF represent the constraint
variables well, but do not match perfectly except in the second constraint. This shows the importance of
considering number of iterations in the reweighting stage.
20 iterations of IPF is sufficient in most cases.

For some applications, an individual-level dataset is required.
For this, we have two possibilities. 
First, we can consider the weights as
probabilities and randomly chose the individuals in a distribution corresponding to the weights.
Second, we can consider the weights as the number of individuals
in this category and create integer weights using
'integerisation' [@Lovelace2013-trs], as discussed subsequently.

## Reweighting with **mipfp** {#mipfp}

```{r, echo=FALSE}
# Morgane: please make this example work with the SimplWorld data
# DONE 
```

```{r, echo=FALSE}
# we have a new column with, at most, as many categories as the product of the number of categories for the variable age
# and 2 (Male or Female). However, in this case, we need to have the cross table of the age and the sex available to 
# proceed this way.
```

**mipfp** is a multidimensional implementation of 
IPF.
The main function of **mipfp** is `Ipfp()`.
Let's test the package on some example data.
The first step is to load the package into the workspace:

```{r}
library(mipfp) # after install.packages("mipfp")
```

To illustrate the use of `Ipfp`, we will create a fictive example.
The example case is as follows: to determine the contingency table 
of a population characterized by categorical variables for age 
(0-17, 18-50, 50+),
gender (male, female) and educational level (level 1 to level 4).
We consider a zone with 50 inhabitants. The classic spatial microsimulation 
problem consists in having all marginal distributions and the cross-tabulated
result (age/gender/education in this case) only for a non-geographical sample. 

We consider the variables in the following 
order: sex (1), age (2) and diploma (3):

```{r}
sex <- c(Male = 23, Female = 27) # n. in each sex category

age <- c(Less18 = 16, Workage = 20, Senior = 14) # age bands

diploma <- c(Level1 = 20, Level2 = 18, Level3 = 6, Level4 = 6) 
```

The population is equal in each constraint (50 people). 
To tell the algorithm 
which elements of the list correspond to which constraint,
a second list with the description 
of the target must be created. 

```{r}
target <- list (sex, age, diploma)
descript <- list (1, 2, 3)
```

Now that all constraint variables have been encoded, let us define 
the initial array to be updated,
also referred as the seed or the weight matrix. The dimension
of this matrix must be identical to that of the constraint tables:
$(2 \times3 \times 4)$. Each cell of the array represents 
a combination of the attributes' values, and thus defines a particular 
category of individuals. 
We assume that it is impossible for an individual being less than 18 
years old to hold a diploma level higher than 2. 
The corresponding cells are then set to 0, while the cells of the 
feasible categories are set to 1.

```{r}
names <- list(names(sex), names(age), names(diploma))
weight <- array (1, c(2,3,4), dimnames = names)
weight[, c("Less18"), c("Level3","Level4")] <- 0
```

Now can execute *Ipfp*:

```{r}
result <- Ipfp(weight, descript, target, iter = 50, print = TRUE, tol = 1e-5)
```

Note that the fit improves rapidly attains the *tol* after 8 
iterations. The `result`contains the final weight matrix
and some information about the convergence. 
We have a resulting table and we can validate the total number of 50 inhabitants 
in the zone. Thanks to the definitions of names in the array, we can easily 
interpret the result. There are a total 50 people and nobody of less than 18 years old
own a diploma level 3 or 4, as desired.

```{r}
result$x.hat # print the result
sum(result$x.hat) # check the total number of persons
```

The quality of the margins with each constraints is contained in the variable `check.margins`
of the resulting list. In our case, we fit all constaints.

```{r}
# printing the resulting margins
result$check.margins
```

This reasoning works zone per zone and we can generate a 3-dimensional weight matrix. 

## Integerisation {#sintegerisation}

Integerisation is the process by which a vector of real numbers
is converted into a vector of integers corresponding to the
individuals present in synthetic spatial microdata.
For the purposes of
this course we will create a function to undertake the simplest of these,
*proportional probabilities*:

```{r}
int_pp <- function(x){
  sample(length(x), size = round(sum(x)), prob = x, replace = T)
}
```

The R function `sample` needs in the order the arguments: the set of objects
that can be chosen, the number of randomly draw and the probability of each 
object to be chosen. Then we can add the argument `replace` to tell R if
the sampling has to be with replacement or not.
To test this function let's try it on the vectors of length 3 described in
code:

```{r}
set.seed(0)
int_pp(x = c(0.333, 0.667, 3))
int_pp(x = c(1.333, 1.333, 1.333))
```

An issue with the
*proportional probabilities* (PP) strategy is that completely unrepresentative
combinations of individuals have a non-zero probability of being sampled. The
method will output $(1, 1, 1, 1)$
once in every 21 thousand runs for $w_1$ and
once every $81$ runs for $w_2$. The same probability is allocated
to all other 81 ($3^4$) permutations.

To overcome this issue Lovelace and Ballas (2012) developed a method which
ensures that any individual with a weight above 1 would be sampled at least once,
making the result $(1, 1, 1, 1)$ impossible in both cases.  This method is
*truncate, replicate, sample* (TRS) integerisation:

```{r}
int_trs <- function(x){
  truncated <- which(x >= 1)
  replicated <- rep(truncated, floor(x[truncated]))
  r <- x - floor(x)
  def <- round(sum(x)) - length(replicated) # deficit population
  if(def == 0){
    out <- replicated
  } else {
    out <- c(replicated,
      sample(length(x), size = def, prob = r, replace = FALSE))
  }
  out
}
```

To see how this new integerisation method and associated 
R function performed, we run it on the same input vectors:

```{r}
set.seed(0)
int_trs(c(0.333, 0.667, 3))
int_trs(c(1.333, 1.333, 1.333))
```

Let's use TRS to
generate spatial microdata for SimpleWorld.  Remember, we already have generated
the weight matrix `weights`.  The only challenge is to save the vector of
sampled individual id numbers, alongside the zone number, into a single object
from which the attributes of these individuals can be recovered.

### Expansion

Two strategies
for doing this are presented in the code below:

```{r}
# Method 1: using a for loop
ints_df <- NULL
for(i in 1:nrow(cons)){
  ints <- int_trs(weights[, i])
  ints_df <- rbind(ints_df, data.frame(id = ints, zone = i))
}

# Method 2: using apply
ints <- unlist(apply(weights, 2, int_trs)) # integerised result
ints_df <- data.frame(id = ints,
  zone = rep(1:nrow(cons), colSums(weights)))
```

Both methods yield the same result for `ints_df`. The only differences being
that Method 1 is perhaps more explicit and easier to understand whilst Method 2
is more concise.

The final remaining step is to re-allocate the attribute data from the
original microdata (contained in `ind`) data back into `ints_df`. We label this process
*expansion*, because it creates a synthetic population.
To do this we use the `inner_join`
function from the recently released **dplyr** package.^[The functions `merge`
from the R's base package and `join` from the **plyr** provide other ways of
undertaking this step. `inner_join` is used in place of `merge` because `merge`
does not maintain row order.  `join` generates the same result, but is slower,
hence the use of `inner_join` from the recently released and powerful **dplyr**
package.]
 Assuming **dplyr** is loaded --- with `library(plyr)`
--- one can read more about join by entering `?inner_join` in R.

```{r, message=FALSE}
library(dplyr) # use install.packages(dplyr) if not installed
ints_df <- inner_join(ints_df, ind_orig)
```

`ints_df` represents the final spatial microdataset, representing the entirety
of SimpleWorld's population of 33 (this can be confirmed with `nrow(ints_df)`).
To select individuals from one zone only is simple using R's subsetting
notation. To select all individuals generated for zone 2, for example, the
following code is used. Note that this is the same as the output generated in
Table 5 at the end of the SimpleWorld chapter --- we have successfully modelled
the inhabitants of a fictional planet, including income!

```{r}
ints_df[ints_df$zone == 2, ]
```


# Glossary

-   **Algorithm**: a series of computer commands executed in a
    specific order for a pre-defined purpose.
    Algorithms process input data and produce outputs.
    
-   **Constraints** are variables used to estimate the number (or weight)
    of individuals in each zone. Also referred to by the longer name of
    **constraint variable**. We tend to use the term **linking variable**
    in this book because they *link* aggregate and individual-level datasets.

-   **Combinatorial optimisation** is an approach to spatial
    microsimulation that generates spatial microdata by randomly
    selecting individuals from a survey dataset and measuring the fit
    between the simulated output and the constraint variables. If the
    fit improves after any particular change, the change is kept.
    Williamson (2007) provides a practical user manual. @Harland2013
    provides a practical demonstration of the method implemented in
    the Java-based Flexible Modelling Framework (FMF).

-   **Data frame**: a type of object (formally referred to as a class)
    in R, data frames are square tables composed of rows and columns of
    information. As with many things in R, the best way to understand
    data frames is to create them and experiment. The following creates
    a data frame with two variables: name and height:

    Note that each new variable is entered using the command `c()` this is
    how R creates objects with the *vector* data class, a one
    dimensional matrix — and that text data must be entered in quote
    marks.

-   **Deterministic reweighting** is an approach to generating spatial
    microdata that allocates fractional weights to individuals based on
    how representative they are of the target area. It differs from
    combinatorial optimisation approaches in that it requires no random
    numbers. The most frequently used method of deterministic
    reweighting is IPF.

-   **For loops** are instructions that tell the computer to run a
    certain set of command repeatedly. `for(i in 1:9) print(i)`, for
    example will print the value of i 9 times. The best way to further
    understand for loops is to try them out.

-   **Iteration**: one instance of a process that is repeated many times
    until a predefined end point, often within an *algorithm*.

-   **Iterative proportional fitting** (IPF): an iterative process
    implemented in mathematics and algorithms to find the maximum
    likelihood of cells that are constrained by multiple sets of
    marginal totals. To make this abstract definition even more
    confusing, there are multiple terms which refer to the process,
    including ‘biproportional fitting’ and ‘matrix raking’. In plain
    English, IPF in the context of spatial microsimulation can be
    defined as *a statistical technique for allocating weights to
    individuals depending on how representative they are of different
    zones*. IPF is a type of deterministic reweighting, meaning that
    random numbers are not needed to generate the result and that the
    output weights are real (not integer) numbers.
    
-   A **linking variable** is a variable that is shared between individual and 
    aggregate-level data. Common examples include age and sex (the linking variables
    used in the SimpleWorld example): questions that are commonly asked in all
    kinds of survey. Linking variables are also referred to as 
    **constraint variables** because they *constrain* the weights for individuals
    in each zone.
    
-   **Microdata** is the non-geographical individual-level dataset from which
    synthetic **spatial microdata** are usually derived. This sample of the
    target population has also been labelled as the 'seed'
    (e.g. Barthelemy and Toint, 2012) and simply the 'survey data' in the academic
    literature. The term microdata is used in this book for its brevity and
    semantic link to spatial microdata.
    
-   The **population base** roughly equivalent to the 'target population',
    used by statisticians to describe the population about whom they wish to
    draw conclusions based on a 'sample population'.
    The sample population, is the group of individuals who
    we have individual-level data for.
    In aggregate-level data, the **population base** is the
    complete set of individuals represented by the counts.
    A common example is the variable "Hours worked":
    only people aged 16 to 74 are generally thought of as working, so, if there is
    no `NA` (no answer) category, the population base is not the same as the total
    population of an area. A common problem faced by people using spatial microsimulation
    methods is incompatibility between aggregate constraints that use different     
    population bases.
    
-   **Population synthesis** is the process of converting input data (generally
    non-geographical **microda** and geographically aggregated 
    **constraint variables**) into **spatial microdata**.
    
-   **Spatial microdata** is the name given to individual-level data allocated
    to mutually exclusive geographical zones (see Figure 5.1 above). Spatial
    microdata is useful because it provides multi-level information, about the
    relationships between individuals and where they live. However, due to the
    high costs of large surveys and restrictions on the release of geocoded
    individual-level data, spatial microdata is rarely available to researchers.
    To overcome this issue, most spatial microsimulation research employs methods
    of **population synthesis** to generate representative spatial microdata.
    
-    **Spatial microsimulation** is the name given to an approach to modelling that
    comprises a series of techniques that
    generate, analyse and model individual-level data allocated to small
    administrative zones. Spatial microsimulation is an approach for
    understanding processes that operate on individual and geographical levels.
    
-    A **weight matrix** is a 2 dimensional array that links non-spatial
    *microdata* to geographical zones. Each row in the weight matrix represents
    an individual and each column represents a zone. Thus, in R notation,
    the weight matrix `w` has dimensions of `nrow(ind)` rows by `nrow(cons)`
    where `ind` and `cons` are the microdata and constraints respectively.
    The value of `w[i,j]` represents the extent to which individual `i` is
    representative of zone `j`. `sum(w)` is the total population of the study area.
    The weight matrix is an efficient way of storing spatial microdata because
    it does not require a new row for every additional individual in the study
    area. For a weight matrix to be converted into spatial microdata, all the
    values of the wieghts must be integers. The conversion of an integer weight
    matrix into an integer weight matrix is known as *integerisation*.
    
# References