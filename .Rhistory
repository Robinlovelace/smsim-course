for(i in 1:ncol(con1)){
weights[which(ind.cat[,i] == 1),j] <- con1[j,i] / ind.agg[j,i]}}
weights
con1
ind.agg
ind.agg <- matrix(rep(colSums(ind.cat), times = nrow(cons)), nrow = nrow(cons) )
############## The IPF part #############
# Re-weighting for constraint 1 via IPF
for (j in 1:nrow(cons)){
for(i in 1:ncol(con1)){
weights[which(ind.cat[,i] == 1),j] <- con1[j,i] / ind.agg[j,i]}}
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
# test results for first row (not necessary for model)
ind.agg[1,1:2] - cons[1,1:2] # should be zero
ind.agg - cons
sum(sqrt((ind.agg - cons)^2))
ind.agg <- matrix(rep(colSums(ind.cat), times = nrow(cons)), nrow = nrow(cons) )
sum(sqrt((ind.agg - cons)^2))
sum(sqrt((ind.agg - cons)^2)) ## the total absolute error
for (j in 1:nrow(cons)){
for(i in 1:ncol(con1)){
weights[which(ind.cat[,i] == 1),j] <- con1[j,i] / ind.agg[j,i]}}
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
# test results for first row (not necessary for model)
sum(sqrt((ind.agg - cons)^2)) ## the total absolute error
ind.agg[1,] - all.msim[1,]
cons[1,] - all.msim[1,]
ind.agg[1,] - cons[1,]
ind.agg[1,] - cons[1,]
for (j in 1:nrow(cons)){
for(i in 1:ncol(con2)){
weights[which(ind.cat[,i] == 1),j] <- con2[j,i] / ind.agg[j,i]}}
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
ind.agg[1,] - cons[1,]
sum(sqrt((ind.agg - cons)^2)) ## the total absolute error
############################################
#### From the IPF-performance-testing github repo
#### https://github.com/Robinlovelace/IPF-performance-testing
############################################
ind <- read.csv("data/simple/ind.csv") # load the individual level data
cons <- read.csv("data/simple/cons.csv") # load aggregate constraints
con1 <- cons[,1:2]
con2 <- cons[,3:4]
# inspect the data we have loaded
ind
cons[1:2,]
source("data/simple/categorise.R") # categorise the individual level variable
ind.cat # take a look at the output
colSums(ind.cat)
weights <- array(1, dim=c(nrow(ind),nrow(cons)))
ini.ws <- weights[,,num.cons+1]
# convert survey data into aggregates to compare with census (3D matix)
ind.agg <- matrix(rep(colSums(ind.cat), times = nrow(cons)), nrow = nrow(cons) )
sum(sqrt((ind.agg - cons)^2)) ## the total absolute error
############## The IPF part #############
# Re-weighting for constraint 1 via IPF
for (j in 1:nrow(cons)){
for(i in 1:ncol(con1)){
weights[which(ind.cat[,i] == 1),j] <- con1[j,i] / ind.agg[j,i]}}
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
# test results for first row (not necessary for model)
ind.agg[1,] - cons[1,]
sum(sqrt((ind.agg - cons)^2)) ## the total absolute error
ind.agg[j,i]
i=1
j=1
for (j in 1:nrow(cons)){
for(i in 1:ncol(con2) + ncol(con1)){
weights[which(ind.cat[,i] == 1),j] <- con2[j,i] / ind.agg[j,i]}}
for (j in 1:nrow(cons)){
for(i in 1:ncol(con2) + ncol(con1)){
weights[which(ind.cat[,i] == 1),j] <- cons[j,i] / ind.agg[j,i]}}
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
ind.agg[1,] - cons[1,]
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
for (j in 1:nrow(cons)){
for(i in 1:ncol(con2) + ncol(con1)){
weights[which(ind.cat[,i] == 1),j] <- con2[j,i] / ind.agg[j,i]}}
con2[j,i]
con2
j
i
for (j in 1:nrow(cons)){
for(i in 1:ncol(con2) + ncol(con1)){
weights[which(ind.cat[,i] == 1),j] <- cons[j,i] / ind.agg[j,i]}}
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights2[,i] * weights[,i])}
ind.agg[1,] - cons[1,]
sum(sqrt((ind.agg - cons)^2)) ## the total absolute error
for (j in 1:nrow(cons)){
for(i in 1:ncol(con1)){
weights[which(ind.cat[,i] == 1),j] <- cons[j,i] / ind.agg[j,i]}}
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
# test results for first row (not necessary for model)
source('~/.active-rstudio-document', echo=TRUE)
source('~/repos/smsim-course/data/simple/etsim.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source("data/simple/categorise.R") # this script must be customised to input data
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/repos/smsim-course/data/simple/etsim.R', echo=TRUE)
source('~/repos/smsim-course/data/simple/etsim.R', echo=TRUE)
source('~/repos/smsim-course/cMap.R', echo=TRUE)
getwd()
ind <- read.csv("data/cakeMap/ind.csv")
cons <- read.csv("data/cakeMap/cons.csv")
# Loading the aggregate dataset, saving as all.msim
getwd() # should be in the smsim-course folder
con1 <- read.csv("data/cakeMap/con1.csv") # age/sex variable
con2 <- read.csv("data/cakeMap/con2.csv") # no car / car
con3 <- read.csv("data/cakeMap/con3.csv") # ns-sec
names(con1)
names(con2)
names(con3)
con2 <- data.frame(cbind(con2[,1] - con2[,2], con2[,2]))
names(con2) <- c("Car", "NoCar")
head(con2)
sum(con1); sum(con2); sum(con3)
c(sum(con1), sum(con2), sum(con3)) / sum(con1) # how much the values deviate from expected
con.pop <- rowSums(con1)
con1 <- round(con1 * con.pop / rowSums(con1))
con2 <- round(con2 * con.pop / rowSums(con2))
con3 <- round(con3 * con.pop / rowSums(con3))
sum(con1); sum(con2); sum(con3); # all the numbers should be equal - this is close enough!
# bind all the data frames together
all.msim <- cbind(con1
,con2
,con3
)
which(all.msim == 0)
range(all.msim) # range of values - there are no zeros
mean(con.pop) # average number of individuals in each zone
# in case there are zeros, set just above 1 to avoid subsequent problems
con1[con1 == 0] <- con2[con2 == 0] <- con3[con3 == 0] <- 0.0001
# previous step avoids zero values (aren't any in this case...)
head(all.msim)
category.labels <- names(all.msim) # define the category variables we're working with
write.csv(all.msim, "data/cakeMap/all.msim.csv", row.names=F)
write.csv(all.msim, "data/cakeMap/cons.csv", row.names=F)
cons <- read.csv("data/cakeMap/cons.csv")
source('~/repos/smsim-course/cMap.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
num.its <- 10 # how many iterations will we run?
source('~/.active-rstudio-document', echo=TRUE)
# defining algorithm objects
start.time <- proc.time()
ints <- as.list(1:nrow(cons)) # Names of integer results, in ordered list
intall <- list(1:nrow(cons))  # Row data, ready to be filled with integer weights
intcat <- ints
intagg <- cons * 0 # Aggregate area stats
#Pops: [,1] is census pop; [,2] will be original; [,3] will be final
ints <- as.list(1:nrow(cons)) # Names of integer results, in ordered list
intall <- list(1:nrow(cons))  # Row data, ready to be filled with integer weights
intcat <- ints
intagg <- cons * 0 # Aggregate area stats
it
i
sum(con1[i,])
for (i in 1:nrow(cons)){
prop.weights[,i] <- wf[,,,it,1][,i] / sum(wf[,,,it,1][,i])
}
wf[,,,it,1][,i]
num.cons
prop.weights[,i] <- wf[,,4,it,1][,i] / sum(wf[,,4,it,1][,i])
prop.weights <- rep(1, nrow(cons))
for (i in 1:nrow(cons)){
prop.weights[,i] <- wf[,,4,it,1][,i] / sum(wf[,,4,it,1][,i])
}
wf[,,4,it,1]
wf[,,4,it,1][,i]
sum(wf[,,4,it,1][,i])
wf[,,4,it,1][,i] / sum(wf[,,4,it,1][,i])
pops <- rowSums(con1)
summary(pops)
fw <- weights[,i,num.cons+1] * weights[,i,1] * weights[,i,2] * weights[,i,3])
fw <- weights[,i,num.cons+1] * weights[,i,1] * weights[,i,2] * weights[,i,3]
prop.weights <- matrix(1, nrow=nrow(ind), ncol=nrow(cons))
pops <- rowSums(con1)
set.seed(0) # Include this line to ensure repeatable results
# Loop to set-up the proportional probability weights
for (i in 1:nrow(cons)){
prop.weights[,i] <- fw[,i] / sum(fw[,i])
}
fw[,i]
fw
source('~/.active-rstudio-document', echo=TRUE)
for (i in 1:nrow(cons)){
prop.weights[,i] <- fw[,i] / sum(fw[,i])
}
head(prop.weights)
colSums(prop.weights) # Check that cumulative probability for individuals in each zone is 1
# Sample individuals based on their proportional probabilities
for (i in 1:nrow(cons)){
ints[[i]] <- sample(which(fw[,i] > 0), size = sum(con1[i,]),
prob=prop.weights[,i], replace = T)
pops$prop_prob[i] <-  length(ints[[i]])
intall[[i]] <- USd[ints[[i]],] # Pulls all other data from index
source("area.cat.R")
intagg[i,]   <- colSums(area.cat)
}
for (i in 1:nrow(cons)){
ints[[i]] <- sample(which(fw[,i] > 0), size = sum(con1[i,]),
prob=prop.weights[,i], replace = T)
popsints1[i] <-  length(ints[[i]])
intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index
intagg[i,]   <- colSums(area.cat)
}
pops <- popints <- rowSums(con1)
for (i in 1:nrow(cons)){
ints[[i]] <- sample(which(fw[,i] > 0), size = sum(con1[i,]),
prob=prop.weights[,i], replace = T)
popsints[i] <- length(ints[[i]])
intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index
intagg[i,]   <- colSums(area.cat)
}
ints <- as.list(1:nrow(cons)) # Names of integer results, in ordered list
intall <- list(1:nrow(cons))  # Row data, ready to be filled with integer weights
intcat <- ints
intagg <- cons * 0 # Aggregate area stats
prop.weights <- matrix(1, nrow=nrow(ind), ncol=nrow(cons))
ints <- as.list(1:nrow(cons)) # Names of integer results, in ordered list
intall <- list(1:nrow(cons))  # Row data, ready to be filled with integer weights
intcat <- ints
intagg <- cons * 0 # Aggregate area stats
prop.weights <- matrix(1, nrow=nrow(ind), ncol=nrow(cons))
pops <- popints <- rowSums(con1)
set.seed(0) # Include this line to ensure repeatable results
# Loop to set-up the proportional probability weights
for (i in 1:nrow(cons)){
prop.weights[,i] <- fw[,i] / sum(fw[,i])
}
head(prop.weights)
colSums(prop.weights) # Check that cumulative probability for individuals in each zone is 1
head(prop.weights[1:5])
colSums(prop.weights) # Check that cumulative probability for individuals in each zone is 1
# Sample individuals based on their proportional probabilities
for (i in 1:nrow(cons)){
ints[[i]] <- sample(which(fw[,i] > 0), size = sum(con1[i,]),
prob=prop.weights[,i], replace = T)
popsints[i] <- length(ints[[i]])
intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index
intagg[i,]   <- colSums(area.cat)
}
for (i in 1:nrow(cons)){
ints[[i]] <- sample(which(fw[,i] > 0), size = sum(con1[i,]),
prob=prop.weights[,i], replace = T)
popints[i] <- length(ints[[i]])
intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index
intagg[i,]   <- colSums(area.cat)
}
for (i in 1:nrow(cons)){
ints[[i]] <- sample(which(fw[,i] > 0), size = sum(con1[i,]),
prob=prop.weights[,i], replace = T)
popints[i] <- length(ints[[i]])
intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index
}
head(intall[[i]])
intall[[i]]$ageband4
AS <- paste0(intall[[i]]$Sex, intall[[i]]$ageband4)
unique(AS)
# matrix for constraint 1 - age/sex
m1 <- model.matrix(~AS-1)
head(cons)
head(m1)
colnames(m1) <- names(cons)[1:12]
head(m1)
summary(rowSums(m1))
intall[[i]]$Car <- as.character(intall[[i]]$Car)
head(cons)
head(m1)
AS <- paste0(ind$Sex, ind$ageband4)
unique(AS)
# matrix for constraint 1 - age/sex
m1 <- model.matrix(~AS-1)
head(cons)
head(m1)
### Proportional probabilities algorithm
### Run in current state after running IPF script in smsim-course repo -
# e.g. etsim.R, simple.R or cakeMap.R
ints <- as.list(1:nrow(cons)) # Names of integer results, in ordered list
intall <- list(1:nrow(cons))  # Row data, ready to be filled with integer weights
intcat <- ints
intagg <- cons * 0 # Aggregate area stats
prop.weights <- matrix(1, nrow=nrow(ind), ncol=nrow(cons))
pops <- popints <- rowSums(con1)
set.seed(0) # Include this line to ensure repeatable results
# Loop to set-up the proportional probability weights
for (i in 1:nrow(cons)){
prop.weights[,i] <- fw[,i] / sum(fw[,i])
}
head(prop.weights[1:5])
colSums(prop.weights) # Check that cumulative probability for individuals in each zone is 1
# Sample individuals based on their proportional probabilities
for (i in 1:nrow(cons)){
ints[[i]] <- sample(which(fw[,i] > 0), size = sum(con1[i,]),
prob=prop.weights[,i], replace = T)
popints[i] <- length(ints[[i]])
intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index
source("data/cakeMap/area.cat.R")
intagg[i,] <- colSums(area.cat)
}
head(intagg)
summary(intall[[i]]$NCakes)
?dplyr
intall[[i]]$avnumcakes <- 1
intall[[i]]$avnumcakes
summary(intall[[i]]$NCakes)
summary(intall[[i]]$avnumcakes[])
intall[[i]]$avnumcakes[intall[[i]]$NCakes == 1] <- 0.5
summary(intall[[i]]$avnumcakes[])
intall[[i]]$avnumcakes[intall[[i]]$NCakes == "<1"] <- 0.5
summary(intall[[i]]$avnumcakes[])
levels(ind$NCakes)
intall[[i]]$avnumcakes[intall[[i]]$NCakes == levels(ind$NCakes)[1]] <- 0.5
intall[[i]]$avnumcakes[intall[[i]]$NCakes == levels(ind$NCakes)[2]] <- 1.5
intall[[i]]$avnumcakes[intall[[i]]$NCakes == levels(ind$NCakes)[3]] <- 4
intall[[i]]$avnumcakes[intall[[i]]$NCakes == levels(ind$NCakes)[4]] <- 8
intall[[i]]$avnumcakes[intall[[i]]$NCakes == levels(ind$NCakes)[5]] <- 0.1
summary(intall[[i]]$avnumcakes[])
cakes <- data.frame(avCake = cons[1] * 0, sdCake = cons[1] * 0)
cakes <- data.frame(avCake = rep(0,nrow(cons)), sdCake = rep(0,nrow(cons)))
cakes[i,1]
summary(fw[ints[[i]][which(duplicated(ints[[i]]) == T)],i]) # Summary statistics of replicated weights in zone 1 (mean should be higher still)
summary(fw[,i]) # Summary statistics of all IPF weights for zone i (mean should be low)
summary(fw[ints[[i]],i]) # Summary statistics of all IPF weights for SELECTED INDIVIDUALS for zone i (mean should be higher)
summary(fw[ints[[i]][which(duplicated(ints[[i]]) == T)],i]) # Summary statistics of replicated weights in zone 1 (mean should be higher still)
head(pops)
source('~/.active-rstudio-document', echo=TRUE)
object.size(intall)
object.size(intall) / 1000000
object.size(intagg)
num.cons
weights <- array(1, dim=c(nrow(ind),nrow(cons)))
ind.agg <- matrix(rep(colSums(ind.cat), nrow(cons)),nrow(cons) )
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
ind.agg
############################################
#### From the IPF-performance-testing github repo
#### https://github.com/Robinlovelace/IPF-performance-testing
############################################
ind <- read.csv("data/simple/ind.csv") # load the individual level data
cons <- read.csv("data/simple/cons.csv") # load aggregate constraints
con1 <- cons[,1:2]
con2 <- cons[,3:4]
# inspect the data we have loaded
ind[1,]
cons[1:2,]
source("data/simple/categorise.R") # categorise the individual level variable
ind.cat # take a look at the output
colSums(ind.cat)
# create weight object and aggregated individual-level data
weights <- array(1, dim=c(nrow(ind),nrow(cons)))
ind.agg <- matrix(rep(colSums(ind.cat), nrow(cons)),nrow(cons) )
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
ind <- read.csv("data/simple/ind.csv") # load the individual level data
cons <- read.csv("data/simple/cons.csv") # load aggregate constraints
con1 <- cons[,1:2]
con2 <- cons[,3:4]
# inspect the data we have loaded
ind[1,]
cons[1:2,]
source("data/simple/categorise.R") # categorise the individual level variable
ind.cat # take a look at the output
colSums(ind.cat)
# create weight object and aggregated individual-level data
weights <- array(1, dim=c(nrow(ind),nrow(cons)))
ind.agg <- matrix(rep(colSums(ind.cat), nrow(cons)),nrow(cons) )
ind.agg
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
ind.agg
ind.agg <- matrix(rep(colSums(ind.cat), nrow(cons)),nrow(cons), byrow=T )
ind.agg
library(gdata) # load package for reading data
ind <- read.xls("msim.xlsx", sheet="SAMPLING")
library(gdata) # load package for reading data
ind <- read.xls("msim.xlsx", sheet="SAMPLING")
ind <- read.xls("worked-eg/msim.xlsx", sheet="SAMPLING")
head(ind)
# load the constraints and check the totals add up
con1 <- read.xls("msim.xlsx", "GENDER")[-1]; sum(con1)
con1 <- read.xls("worked-eg/msim.xlsx", "GENDER")[-1]; sum(con1)
con2 <- read.xls("worked-eg/msim.xlsx", "AGE")[-1]; sum(con2)
con3 <- read.xls("worked-eg/msim.xlsx", "ETHNICITY")[-1]; sum(con3)
num.cons <- 3  # n. constraints - can set automatically: length(which(grepl("con[1-9]",ls()))))
cons <- cbind(con1, con2, con3)
cat.names <- names(cons)
gender.cat <- model.matrix(~ind$GENDER -1 )
age.cat <- model.matrix(~ind$AGE -1)
eth.cat <- model.matrix(~ind$ETHNICITY - 1)
ind.cat <-  cbind(gender.cat, age.cat, eth.cat)
names(ind.cat) <- cat.names
weights <- array(dim=c(nrow(ind),nrow(cons),num.cons+1))
weights[,,1] <- 1 # sets initial weights to 1
ind.agg <- array(dim=c(nrow(cons),ncol(cons),num.cons+1))
for (i in 1:nrow(cons)){
ind.agg[i,,1]   <- colSums(ind.cat) * weights[1,i,1]}
ind.agg[,,1] # look at what we've created - individual level data comparable w. cons
for (j in 1:nrow(cons)){
for(i in 1:ncol(con1)){
weights[which(ind.cat[,i] == 1),j,2] <- con1[j,i] / ind.agg[j,i,1]}}
for (i in 1:nrow(cons)){ # convert con1 weights back into aggregates
ind.agg[i,,2]   <- colSums(ind.cat * weights[,i,1] * weights[,i,2])}
# test results for first row (not necessary for model)
ind.agg[1,,2] - cons[1,] # should be zero for age/sex
cor(as.numeric(as.vector(ind.agg[,,2])), as.numeric(as.matrix(cons))) # how good is the correlation (fit)
for (j in 1:nrow(cons)){
for(i in 1:ncol(con2) + ncol(con1)){
weights[which(ind.cat[,i] == 1),j,3] <- cons[j,i] / ind.agg[j,i,2]}}
for (i in 1:nrow(cons)){ # convert con2 back into aggregate
ind.agg[i,,3] <- colSums(ind.cat * weights[,i,1] * weights[,i,2] * weights[,i,3])}
ind.agg[1,,3] - cons[1,] # should be close to zero for new constraint
cor(as.numeric(as.vector(ind.agg[,,3])), as.numeric(as.matrix(cons))) # how good is the correlation (fit)
for (j in 1:nrow(cons)){
for(i in 1:ncol(con3) + ncol(con1) + ncol(con2)){
weights[which(ind.cat[,i] == 1),j,4] <- cons[j,i] / ind.agg[j,i,3]}}
for (i in 1:nrow(cons)){ # convert con3 back into aggregate
ind.agg[i,,4]   <- colSums(ind.cat * weights[,i,1] * weights[,i,2] * weights[,i,3] * weights[,i,4])}
ind.agg[1:3,,4] - cons[1:3,] # test the result
cor(as.numeric(as.vector(ind.agg[,,4])), as.numeric(as.matrix(cons))) # how good is the correlation (fit)
# you get a perfect fit between constraint data and results of model
# why? because of the final weights:
fw <- weights[,,1] * weights[,,2] * weights[,,3] * weights[,,4]
head(fw) # cols are zones, rows are individuals
for (i in 1:nrow(cons)){
if(max(f[,i]) == 0) f[which.max(fw[,i]),i] <- 1 # ensures model will run in case max(i5.w5 < 1) thanks to Eveline van Leeuwen
ints[[i]] <- rep(which(fw[,i] > 0), f[,i])
s <- sample(which(fw[,i] > 0), size = sum(con1[i,]) - sum(f[,i]) , # sample using decimal weights to 'top up' selection
prob=d[,i], replace = F)
ints[[i]] <- c(ints[[i]], s) # add the sampled population to the one selected from truncated weights
intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index
source("areaCat.R") # save the aggregate data
intagg[i,] <- colSums(area.cat)
}
intall <- ints <- as.list(1:nrow(cons)) # Names of integer indices (ints), and integer populations (intall) in ordered list
intagg <- cons * 0 # Aggregate area stats - set to 0 to avoid confusion
f <- floor(fw) # truncated weights
d <- fw - f
set.seed(0) # Include this line to ensure repeatable results
for (i in 1:nrow(cons)){
if(max(f[,i]) == 0) f[which.max(fw[,i]),i] <- 1 # ensures model will run in case max(i5.w5 < 1) thanks to Eveline van Leeuwen
ints[[i]] <- rep(which(fw[,i] > 0), f[,i])
s <- sample(which(fw[,i] > 0), size = sum(con1[i,]) - sum(f[,i]) , # sample using decimal weights to 'top up' selection
prob=d[,i], replace = F)
ints[[i]] <- c(ints[[i]], s) # add the sampled population to the one selected from truncated weights
intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index
source("areaCat.R") # save the aggregate data
intagg[i,] <- colSums(area.cat)
}
source("worked-eg/areaCat.R") # save the aggregate data
for (i in 1:nrow(cons)){
if(max(f[,i]) == 0) f[which.max(fw[,i]),i] <- 1 # ensures model will run in case max(i5.w5 < 1) thanks to Eveline van Leeuwen
ints[[i]] <- rep(which(fw[,i] > 0), f[,i])
s <- sample(which(fw[,i] > 0), size = sum(con1[i,]) - sum(f[,i]) , # sample using decimal weights to 'top up' selection
prob=d[,i], replace = F)
ints[[i]] <- c(ints[[i]], s) # add the sampled population to the one selected from truncated weights
intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index
source("worked-eg/areaCat.R") # save the aggregate data
intagg[i,] <- colSums(area.cat)
}
intall
intall.df <- cbind(intall[[1]], zone = 1)
head(intall.df)
for(i in 2:nrow(cons)){ # run for all zones with 1:length(intall)
intall.df <- rbind(intall.df, cbind(intall[[i]], zone = i))
}
summary(intall.df[ intall.df$zone == 1, ]) # test the output
summary(intall.df[ intall.df$zone == 3, ]) # test the output
summary(intall.df)
colSums(cons)
ind.agg <- array(dim=c(nrow(cons),ncol(cons),num.cons + 1))
for (i in 1:nrow(cons)){ #
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
############################################
#### From the IPF-performance-testing github repo
#### https://github.com/Robinlovelace/IPF-performance-testing
############################################
ind <- read.csv("data/simple/ind.csv") # load the individual level data
cons <- read.csv("data/simple/cons.csv") # load aggregate constraints
con1 <- cons[,1:2]
con2 <- cons[,3:4]
# inspect the data we have loaded
ind[1,]
cons[1:2,]
colSums()
source("data/simple/categorise.R") # categorise the individual level variable
ind.cat # take a look at the output
colSums(ind.cat)
# create weight object and aggregated individual-level data
weights <- array(1, dim=c(nrow(ind),nrow(cons)))
ind.agg <- array(dim=c(nrow(cons),ncol(cons),num.cons + 1))
for (i in 1:nrow(cons)){ #
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
ind.agg <- array(dim=c(nrow(cons),ncol(cons)))
for (i in 1:nrow(cons)){ #
ind.agg[i,]   <- colSums(ind.cat * weights[,i])}
array(dim=c(nrow(cons), ncol(cons)))
ind.agg <- matrix ( rep ( colSums ( ind . cat ) , nrow ( cons )) , nrow ( cons ) )
ind.agg <- matrix ( rep ( colSums ( ind.cat ) , nrow ( cons )) , nrow ( cons ) )
ind.agg
ind.agg <- matrix (rep(colSums ( ind.cat ) , nrow (cons)) , nrow (cons), byrow = T )
source('~/repos/smsim-course/simple.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
