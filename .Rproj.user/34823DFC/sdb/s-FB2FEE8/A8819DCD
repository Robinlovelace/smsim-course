{
    "contents" : "'Spatial microsimulation' in R: allocating individuals to zones\n========================================================\n\nThis reproducible code shows how to use IPF to generate lists of individuals for individual zones using IPF.\nThe input datasets are 3 constraint variables and an individual level dataset.\nWe will work through the entire process to show how spatial microsimulation can be done in R.\n\n## Loading the input data\nThe first stage is to load the input data, from an Excel spreadsheet in this case.\n\n```{r}\n# install.packages(\"gdata\") # install gdata package if not already installed\nlibrary(gdata) # load package for reading data\nind <- read.xls(\"worked-eg/msim.xlsx\", sheet=\"SAMPLING\")\nhead(ind)\n\n# load the constraints and check the totals add up\ncon1 <- read.xls(\"worked-eg/msim.xlsx\", \"GENDER\")[-1]; sum(con1)\ncon2 <- read.xls(\"worked-eg/msim.xlsx\", \"AGE\")[-1]; sum(con2)\ncon3 <- read.xls(\"worked-eg/msim.xlsx\", \"ETHNICITY\")[-1]; sum(con3)\nnum.cons <- 3  # n. constraints - can set automatically: length(which(grepl(\"con[1-9]\",ls()))))\ncons <- cbind(con1, con2, con3)\ncat.names <- names(cons)\n```\n\n## Converting the individual level data into a 0/1 \"model matrix\" for aggregation\n\nIn order to compare individual level data with aggregate constraints,\nthe aggregate data must be converted into a flat form, with counts\nfor each of the categorise in the dataset:\n\n```{r}\n# creating 0/1 matrix representation of ind. data\ngender.cat <- model.matrix(~ind$GENDER -1 )\nage.cat <- model.matrix(~ind$AGE -1)\neth.cat <- model.matrix(~ind$ETHNICITY - 1)\nind.cat <-  cbind(gender.cat, age.cat, eth.cat)\nnames(ind.cat) <- cat.names\n```\n\n\n## Create the weight matrix and initialise\nIPF *reweights* individuals for each zone (A to C in this case).\nFor this we must first create a weight matrix.\nIn fact, this will be a *weight array*, with a 2D matrix for each\nconstraint, allowing for easy iteration through the constraints.\n\n```{r}\nweights <- array(dim=c(nrow(ind),nrow(cons),num.cons+1)) \nweights[,,1] <- 1 # sets initial weights to 1\n```\n\n## Create aggregated output matrix and add values from individual inputs\nThis stage creates an array for the aggregated outputs.\nNotice that `ind.agg` has the same dimension as `cons`,\nallowing for direct comparison between the two.\nThis is the key to IPF!\n\n```{r}\nind.agg <- array(dim=c(nrow(cons),ncol(cons),num.cons+1))\nfor (i in 1:nrow(cons)){\n  ind.agg[i,,1]   <- colSums(ind.cat) * weights[1,i,1]}\nind.agg[,,1] # look at what we've created - individual level data comparable w. cons\n```\n\n\n\n\n# The IPF part #############\n\nNow that all the data and objects used for the model have been set-up,\nwe are ready to run the model. We constrain by one constraint at a time.\n\n```{r}\n# Re-weighting for constraint 1 via IPF \nfor (j in 1:nrow(cons)){\n  for(i in 1:ncol(con1)){\n    weights[which(ind.cat[,i] == 1),j,2] <- con1[j,i] / ind.agg[j,i,1]}}\nfor (i in 1:nrow(cons)){ # convert con1 weights back into aggregates\n  ind.agg[i,,2]   <- colSums(ind.cat * weights[,i,1] * weights[,i,2])}\n# test results for first row (not necessary for model)\nind.agg[1,,2] - cons[1,] # should be zero for age/sex\ncor(as.numeric(as.vector(ind.agg[,,2])), as.numeric(as.matrix(cons))) # how good is the correlation (fit)\n```\n\n## Second constraint\n```{r}\nfor (j in 1:nrow(cons)){\n  for(i in 1:ncol(con2) + ncol(con1)){\n    weights[which(ind.cat[,i] == 1),j,3] <- cons[j,i] / ind.agg[j,i,2]}}  \nfor (i in 1:nrow(cons)){ # convert con2 back into aggregate\n  ind.agg[i,,3] <- colSums(ind.cat * weights[,i,1] * weights[,i,2] * weights[,i,3])}\nind.agg[1,,3] - cons[1,] # should be close to zero for new constraint\ncor(as.numeric(as.vector(ind.agg[,,3])), as.numeric(as.matrix(cons))) # how good is the correlation (fit)\n```\n\n## Third constraint\n```{r}\nfor (j in 1:nrow(cons)){\n  for(i in 1:ncol(con3) + ncol(con1) + ncol(con2)){\n    weights[which(ind.cat[,i] == 1),j,4] <- cons[j,i] / ind.agg[j,i,3]}}\nfor (i in 1:nrow(cons)){ # convert con3 back into aggregate\n  ind.agg[i,,4]   <- colSums(ind.cat * weights[,i,1] * weights[,i,2] * weights[,i,3] * weights[,i,4])}\nind.agg[1:3,,4] - cons[1:3,] # test the result\n```\n\n## Improvements in model fit\n\nNotice that the fit of the model improves from one constraint to the next.\nWhat is the final model fit?\n\n```{r}\ncor(as.numeric(as.vector(ind.agg[,,4])), as.numeric(as.matrix(cons))) # how good is the correlation (fit)\n# you get a perfect fit between constraint data and results of model\n# why? because of the final weights:\nfw <- weights[,,1] * weights[,,2] * weights[,,3] * weights[,,4]\nhead(fw) # cols are zones, rows are individuals\n```\n\n# Integerisation phase ###################\n\nWe have allocated weights to the individuals for a good (perfect)\nmodel fit. These are the *maximum likelihood* or *maximum entropy* values\nto match the individuals with the zones.\nThe final stage is to convert this *weight matrix* into a list of individuals for each zone.\n\n## Setting up objects for the integerisation phase\n\n```{r}\nintall <- ints <- as.list(1:nrow(cons)) # Names of integer indices (ints), and integer populations (intall) in ordered list\nintagg <- cons * 0 # Aggregate area stats - set to 0 to avoid confusion\nf <- floor(fw) # truncated weights\nd <- fw - f\n\nset.seed(0) # Include this line to ensure repeatable results\n```\n\n## Integerisation loop\n\nHere we sample individuals based on their weights.\nThis is the Truncate Replicate Sample (TRS) method described by\nLovelace and Ballas (2011).\n\n```{r}\nfor (i in 1:nrow(cons)){\n  if(max(f[,i]) == 0) f[which.max(fw[,i]),i] <- 1 # ensures model will run in case max(i5.w5 < 1) thanks to Eveline van Leeuwen\n  ints[[i]] <- rep(which(fw[,i] > 0), f[,i])\n  s <- sample(which(fw[,i] > 0), size = sum(con1[i,]) - sum(f[,i]) , # sample using decimal weights to 'top up' selection\n              prob=d[,i], replace = F) \n  ints[[i]] <- c(ints[[i]], s) # add the sampled population to the one selected from truncated weights\n  intall[[i]] <- ind[ints[[i]],] # Pulls all other data from index\n  source(\"worked-eg/areaCat.R\") # save the aggregate data\n  intagg[i,] <- colSums(area.cat) \n}\n```\n\n## The results\n\nWhat is the result of this?\nA list of individuals for each zone. Let's take a look at all of the model output.\n\n```{r}\nintall\n```\n\n## Rearranging the output into a single data frame\nFor ease of analysis, it is best to have all the output\nindividuals in a single data frame, with a new column added to show which\nzone they belong to:\n\n```{r}\ndo.call(rbind, )\n\nintall.df <- cbind(intall[[1]], zone = 1)\nhead(intall.df)\nfor(i in 2:nrow(cons)){ # run for all zones with 1:length(intall)\n  intall.df <- rbind(intall.df, cbind(intall[[i]], zone = i))\n}\nsummary(intall.df[ intall.df$zone == 1, ]) # test the output\nsummary(intall.df[ intall.df$zone == 3, ]) # test the output\nsummary(intall.df)\n```\n\n## The impact of integerisation on model fit\nIntegerisation usually introduces some error. Let's see how much:\n\n```{r}\ncor(as.numeric(as.matrix(intagg)), as.numeric(as.matrix(cons)))\n```\n\nThe answer is NOT A LOT! The integerisation strategy is good at selecting\nappropriate individuals for each zone.\n\n\n",
    "created" : 1409673639824.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "481158958",
    "id" : "A8819DCD",
    "lastKnownWriteTime" : 1409700435,
    "path" : "~/repos/smsim-course/worked-eg/smsim-example.Rmd",
    "project_path" : "worked-eg/smsim-example.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}