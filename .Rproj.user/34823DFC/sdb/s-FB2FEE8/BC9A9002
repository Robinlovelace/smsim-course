{
    "contents" : "\\documentclass[a4paper, 11pt, twoside]{article}  \n\\usepackage[hcentering,bindingoffset=10mm,margin=20mm]{geometry}\n\\usepackage{graphicx}\n\\graphicspath{{figures/}}  % Location of the graphics files\n\\usepackage{makeidx}\n% \\graphicspath{/figures}  % Location of the graphics files\n\\usepackage{multirow}\n% Making R code work!\n\\usepackage{listings}\n\\usepackage{color}\n\\usepackage{hyperref}\n\\usepackage{booktabs}\n\\usepackage{float} % allows forcing of float position\n% \\restylefloat{table}\n\\usepackage{bm}\n\\usepackage[parfill]{parskip}\n% begins paragraphs with an empty line rather than an indent\n\n% Add toc to contents\n\\usepackage[nottoc,numbib]{tocbibind}\n\n\\hypersetup{urlcolor=blue, colorlinks=false, hypertexnames=true}  % Colours hyperlinks in blue, but this can be distracting \n\\usepackage[capitalise]{cleveref}\n\\definecolor{dkgreen}{rgb}{0,0.6,0}\n\\definecolor{gray}{rgb}{0.5,0.5,0.5}\n\\definecolor{mauve}{rgb}{0.58,0,0.82}\n\n\\crefname{lstlisting}{listing}{listings}\n\\Crefname{lstlisting}{Listing}{Listings}\n\n\\lstset{ %\n  language=R,                % the language of the code\n   basicstyle=\\normalsize\\ttfamily,           % the size of the fonts that are used for the code\n%   numbers=left,                   % where to put the line-numbers\n%   numberstyle=\\tiny\\color{gray},  % the style that is used for the line-numbers\n%   stepnumber=2,                   % the step between two line-numbers. If it's 1, each line\n                                  % will be numbered\n%   numbersep=5pt,                  % how far the line-numbers are from the code\n%   backgroundcolor=\\color{white},      % choose the background color. You must add \\usepackage{color}\n%   showspaces=false,               % show spaces adding particular underscores\n%   showstringspaces=false,         % underline spaces within strings\n%   showtabs=false,                 % show tabs within strings adding particular underscores\n   frame=false,                   % adds a frame around the code\n   rulecolor=\\color{white},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))\n%   tabsize=2,                      % sets default tabsize to 2 spaces\n%   captionpos=b,                   % sets the caption-position to bottom\n%   breaklines=true,                % sets automatic line breaking\n%   breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace\n%   title=\\lstname,                   % show the filename of files included with \\lstinputlisting;\n                                  % also try caption instead of title\n  keywordstyle=\\color{blue},          % keyword style\n  commentstyle=\\color{dkgreen},       % comment style\n  stringstyle=\\color{mauve},         % string literal style\n  escapeinside={\\%*}{*)},            % if you want to add a comment within your code\n  morekeywords={*,...}               % if you want to add more keywords to the set\n} \n\n% Include any extra LaTeX packages required\n\\usepackage[round,]{natbib}  % Use the \"Natbib\" style for the references\n\\usepackage{verbatim}  % Needed for the \"comment\" environment to make LaTeX comments\n\\usepackage{wallpaper}\n\\usepackage{cases}\n\\usepackage{wrapfig}\n\\makeindex\n% \\renewcommand{\\includegraphics}[2][]{\\fbox{#2}} %omits images\n\\begin{document}\n \n\\title{Introducing spatial microsimulation with R: a practical}\n% \\author{Robin Lovelace --- R.Lovelace at. Leeds. ac. uk}\n\\pagestyle{myheadings}\n\\author{Lovelace, Robin\\\\\n\\texttt{r.lovelace@leeds.ac.uk}}\n\\maketitle\n\n\\tableofcontents\n\n\\newpage\n\\section{Foundations}\n\nThis practical teaches the basic theory and practice of `spatial microsimulation'\nusing the popular free software package R. The term microsimulation means\ndifferent things in different disciplines, so it is important to be clear\nat the outset what we will and will not be covering.\n\n\\emph{We will} be learning how to create \\emph{spatial microdata}, the\nbasis of all spatial microsimulation models, using \\emph{iterative\nproportional fitting} (IPF). IPF is\nan efficient method for allocating individuals from a\nnon-spatial dataset to geographical zones, analogous to the\n`Furness method' in transport modelling, but with more constraints.\nThere are other ways of generating\nspatial microdata but, as far as the author is\naware,\\footnote{There is much need\nto compare different methods of generating spatial microdata, for example by\nbuilding on the work of \\citep{harland2012}. A project for doing\nthese tests in a transparent way has been started at\n\\href{https://github.com/Robinlovelace/IPF-performance-testing}{github.com/Robinlovelace/IPF-performance-testing}\nand there are probably many opportunities for improving the computational efficiency\nof the code presented in this course --- please get in touch if you would\nlike to contribute.}\nthis is the most effective and flexible for many applications.\nAn alternative approach using the open source `Flexible Modelling Framework'\nprogram is described in detail, with worked examples, by \\citet{harland2013microsimulation}.\n\n\\emph{We will not} be learning `dynamic spatial microsimulation'\n\\citep{Ballas2005b}: once the\nspatial microdata have been generated and \\emph{integerised}, it is up to the\nuser how they are used --- be it in an agent based model or as a basis for\nestimates of income distributions at the local level or whatever.\n\nWe thus define spatial microsimulation narrowly in this tutorial as the process\nof generating \\emph{spatial microdata} (more on this below). The term\ncan also be used to describe a wider approach that harnesses individual-level data\nallocated to zones for investigating phenomena that vary over space and between individuals\nsuch as income inequality or energy overconsumption. In both cases, the generation\nof spatial microdata is the critical element of the modelling process so the skills\nlearned in this tutorial will provide a firm foundation for further work.\n\nOne of the tricky things about spatial microsimulation for newcomers is its use\nof specialist language. It is important to know exactly what is meant by\n`spatial microdata' and other technical terms. To this end we have created a\nglossary that provide succinct definitions (see \\cref{gloss}). Any term that is\n\\emph{italicised} in the text has a glossary entry.\n\n\\begin{wrapfigure}{2}{11cm}\n\\begin{center}\n   \\includegraphics[width=10.5cm]{rstudio}\n\\end{center}\n \\caption{The RStudio user interface. Note the project title `smsim-course' in\nthe top right and the `Files' tab at the top of the left hand window.}\n\\label{frstudio}\n\\end{wrapfigure}\n\n\\subsection{Prerequisites for this practical}\n\nThis practical uses the statical analysis and modelling software\nR. We suggest you install and take a look at this powerful program\nbefore getting started. We recommend using R within RStudio \\cref{frstudio}, which makes\nusing R much easier. Instructions to install both R and RStudio can be found\nat \\href{http://www.rstudio.com/ide/download/desktop}{rstudio.com}.\n% Could add screenshot of Dl ZIP here!!!\n\nThe other prerequisite for the course is downloading the example data.\nThese can be downloaded in a single zip file which can be found on the\ncourse's GitHub repository:\\footnote{GitHub\nis used to serve this .zip file because if anything changes\nin the tutorial, the .zip file from the same location will\nautomatically be updated. The other advantages of GitHub are its\nvisibility --- a pdf of this tutorial, for example,\nis kept up-to-date at \n\\href{https://github.com/Robinlovelace/smsim-course/blob/master/handout.pdf}{github.com/Robinlovelace/smsim-course}\nin the `handout.pdf' file --- accessibility and encouragement of evolution: to contribute\nto this project all one needs to do is\n`\\href{https://help.github.com/articles/fork-a-repo}{fork}' it.\n} \\\\\n\\href{https://github.com/Robinlovelace/smsim-course}{github.com/Robinlovelace/smsim-course}.\nClick on the ``Download ZIP'' button to the right of this page and extract\nthe folder into your desktop or other suitable place on your computer.\n\nOnce the folder has been successfully extracted open it in\nyour browser and take a look around. You will find a number of files\nand two sub-folders\nentitled `data', and `figures'. When you get to the\nsections that use R code, it is useful for R to operate from\nwithin the smsim-course-master folder. Probably the best way\nto set this up is to open the file `smsim-course.Rproj' from\nwithin RStudio (\\cref{frstudio}).\nTry this now and click on the `Files' tab in\nthe bottom right hand window of RStudio.\nBefore using the power of R in RStudio it's worth\nunderstanding a bit about `IPF', the algorithm we will use to generate\nthe synthetic population or `spatial microdata' (\\cref{fmsim-schema}).\n\n\\begin{figure}[H]\n\\begin{center}\n\\includegraphics[width=7cm]{msim-schema}\n\\end{center}\n\\caption{Schema of iterative proportional fitting (IPF) and combinatorial\noptimisation\nin the wider context of the availability of different data formats and spatial\nmicrosimulation. \\label{fmsim-schema}}\n\\end{figure}\n\n\\subsection{Learning by doing}\nAs \\citet[xxii]{kabacoff2011r} put it regarding R, ``the best\nway to learn is to experiment'' and the same applies to spatial microsimulation.\nWe believe you will learn the technique/art best not by reading about it, \nbut by doing it. Mistakes are inevitable in any challenging task and should not\nbe discouraged. In fact it is by making blunders, identifying and then correcting them\nthat many people learn best. Think of someone learning to skate: no one ever\npicks up a skateboard for the first time being able to `surf the sidewalks'. It\ntakes time, patience and plenty of falls before you master the art.\nThe same applies to spatial microsimulation.\n\nSpatial microsimulation works by taking \\emph{microdata} at the individual level\nand using aggregate-level constraints to allocate these individuals to zones.\nThe two main methods are \\emph{deterministic reweighting} and \\emph{combinatorial optimisation}.\nThis practical takes the former approach using a process called \\emph{iterative proportional fitting}\n(IPF). IPF is used to increase the weights of individuals who are representative\nof the target area and reduce the weights of individuals who are relatively\nrare \\citep{Lovelace2013-trs}.\nThe output is a \\emph{spatial microdataset}.\n\nA schematic of the process is shown in \\cref{fmsim-schema}.\nTake a look at the image and think about the process. But we don't want to get\nbogged down in theory or applications in this course: we want to `get our\nhands dirty'. Next we will do just that, by applying the process to some\nexample data.\n\n\\subsection{Input data} \\label{s:theory}\n\nLet's start with two very basic datasets. To aid understanding, we will primarily do\nthe reweighting by hand to understand the process before automating it on the computer.\nAs outlined in Figure~\\ref{fmsim-schema},\nthere are two input files required to perform spatial microsimulation:\n\\begin{itemize}\n  \\item Survey data - information about individuals\n  \\item Geographically aggregated (`wide') data - typically aggregated Census tables\n\\end{itemize}\n\nTable \\ref{t:w} shows 5 individuals, who are defined by two constraint variables:\nage and sex. This is analogous to the survey data shown in Figure~\\ref{fmsim-schema}.\nTable \\ref{t:m} presents this same data in aggregated form, whose margin totals can\nbe used in the IPF procedure described shortly. Note that individual 3 is highlighted in\nblue and bold for future reference.\n\n\\begin{table}[h]\n% {r}{10cm}\n\\caption{Hypothetical input microdata (`survey data')\n(the original\nweights set to one). The bold and blue value is used subsequently for\nillustrative purposes.\n}\n\\label{t:w}\n\\begin{center}\n \\begin{tabular}{llll}\n\\toprule\n{Individual } & {Age} & {Sex}  & {Weight} \\\\\n\\midrule\n1 & 59 & Male  & 1 \\\\\n2 & 54 & Male & 1 \\\\\n3 & {35} & {Male} & \\textbf{\\color{blue}1} \\\\\n4 & 73 & Female & 1 \\\\\n5 & 49 & Female & 1 \\\\\n% 1 & 59 & m \\\\\n% 2 & 54 & m \\\\\n% 3 & 35 & m \\\\\n% 4 & 73 & f \\\\\n% 5 & 49 & f \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\\begin{table}[htbp]\n\\centering\n\\caption[The aggregated results of the weighted\nmicrodata set]{The aggregated results of the weighted\nmicrodata set ($m$ --- think ``m'' for Modified or siMulated).\nNote, these values are updated by each new set of weights.\n% so change after each iteration.\n% hence the number ``(1)''\n% --- this table is for the initial weight.\n}\n\n\\begin{tabular}{cllll}\\toprule\nMarginal totals&  & \\multicolumn{2}{c}{$j$} & \\\\\n& Age/sex & Male & Female & T \\\\ \\midrule\n\\multirow{2}{*}{$i$} & Under-50 & \\textbf{\\color{blue}1} & 1 & \\textbf{\\color{blue}2}\\\\\n& Over-50 & 2 & 1 &3 \\\\\n& T & 3 & 2 &5\\\\\n\\bottomrule\n\\end{tabular}\n\\label{t:m}\n\\end{table}\n\nTable \\ref{t:s} contains data for a single area. This is analogous to the\ngeographically aggregated 'wide' data shown in Figure~\\ref{fmsim-schema}.\n\n\\begin{table}[H]\n\\centering{\n\\caption{Small area constraints (geographically aggregated or `wide' input data).\nNote that although individual 3 fits into both ``under 50'' and ``male'' categories,\nonly the value of the former is highlighted as age is the first constraint. \\label{t:s}}\n\\begin{tabular}{cllll}\n\\toprule\nConstraint $\\Rightarrow$ & \\multicolumn{2}{c}{$i$}& \\multicolumn{2}{c}{$j$}\\\\\nCategory $\\Rightarrow$ & $i_1$ & $i_2$ & $j_1$ & $j_2$ \\\\\nArea $\\Downarrow$  & Under-50 & Over-50 &  Male & Female\\\\\n1  & \\textbf{\\color{blue}8} & 4 & 6 & 6\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n\nNow, it will be recognised by the astute reader that the data in\nTables \\ref{t:m} and \\ref{t:s} are incompatible; they are\ntwo matrices with different dimensions ($3 \\times 3$ and $1 \\times 4$ respectively).\nTo solve this problem we can rearrange the small area census constraints data\n($s$, presented in Table \\ref{t:s}) to match the dimensions of\n$m(1)$ (table \\ref{t:m}).\n\\Cref{t:s2} illustrates $s$ in a different form.\nNote that although we are ignorant of the internal values of this matrix,\nit is of the same dimension as $m(1)$ and the marginal totals (denoted ``T'' for\ntotal) can be directly compared. This is the basis of the method and provides a\nway to systematically compare individual level data with geographic aggregates.\n\n\\begin{table}[h]\n\\centering\n\\caption[Small area constraints expressed as marginal totals]{Small\narea constraints (matrix $s$) re-arranged to be the same dimension as $m$ (Table \\ref{t:m}).\n``s'' here stands for Small area data or conStraint. Note only\nthe marginal totals are known. The internal cell\nvalues (represented by ``?'') must be estimated by spatial microsimulation.\n}\n\\begin{tabular}{cllll}\\toprule\nMarginal totals&  & \\multicolumn{2}{c}{$j$} & \\\\\n& Age/sex & Male & Female & T\\\\ \\midrule\n\\multirow{2}{*}{$i$} & Under-50 & \\textbf{?} & ? & \\textbf{\\color{blue}8}\\\\\n& Over-50 & ? & ? &4 \\\\\n& T & 6 & 6 &12\\\\\n\\bottomrule\n\\end{tabular}\n\\label{t:s2}\n\\end{table}\n\nWith our input data in place, we can begin our first iteration.\n\n\\subsection{The IPF equation} \\label{ipfeq}\n\n% \\begin{wrapfigure}{R}{9cm}\n%    \\includegraphics[width=8cm]{equation1}\n%  \\caption{\\Cref{eq:ipf} in word form.}\n% \\label{feq}\n% \\end{wrapfigure}\n\nUsing the tables $m$ and $s$ presented above we readjust the weights of the\nindividuals so that:\n\n\\begin{itemize}\n% \\item If there are exactly the same number of \n  \\item Individuals who are rare in the geographic zone of interest are given a smaller weight (we need fewer of them).\n  \\item Individuals who are common in the zone of interest are given a larger weight (we need more of them).\n  \\item Their sum equals the total population of the geography of interest (12 in our example).\n\\end{itemize}\n\nFor each constraint the current weight (Table~\\ref{t:m}) is multiplied by the respective\ntotal from the geographically aggregated (census) table (Table~\\ref{t:s}) and\ndivided by the corresponding marginal total of the survey data (see \\cref{t:m}).\nFor example, using the age constraint the new weight of individual 3 is calculated\nbased on the following numbers: the initial weight is set as 1, the number of\nindividuals under 50 in the geographically aggregated (Census) data is 8 (see Table \\ref{t:s})\nand the number\nof individuals under 50 in the survey data is 2 (see Table~\\ref{t:m}). Thus the new weight is\n$1 \\times \\frac{8}{2}$. These figures\nare highlighted in \\textbf{\\color{blue}bold and blue} in the preceding tables.\nThe same process is used to calculate the new weights for all individuals --- remember\nwe are only constraining by age for now.\n% Test your understanding by calculating\n% the value of the ? symbols in table~\\ref{t:new-weights-part-complete} below.\n% \n% \\begin{table}[htbp]\n% \\centering\n% \\caption{Partially complete reweighted microdata.}\n% \\begin{tabular}{lllll}\n% \\toprule\n% {Individual} & {Sex} & {age-group} & {Weight} &\n% {New weight, $w(2)$} \\\\ \\midrule\n% 1 & Male & Over-50 & 1 & ? \\\\\n% 2 & Male & Over-50 & 1 & ? \\\\\n% 3 & Male & Under-50 & 1 & $1 \\times \\frac{\\color{blue}8}{\\color{blue}2} = 4$ \\\\\n% 4 & Female & Over-50 & 1 & ? \\\\\n% 5 & Female & Under-50 & 1 & ? \\\\\n% \\bottomrule\n% \\end{tabular}\n% \\label{t:new-weights-part-complete}\n% \\end{table}\n% \nThis process of \\emph{reweighting} is done one constraint at a time and can\nbe described more succinctly in the form of a word equation\n\\cref{eq:ipf-word}, and more generally by \\cref{eq:ipf} for constraint $i$ (age):\n\n\\begin{equation}\n  Weight_{New} = Weight_{Current} \\times \\frac{Census_{Constraint \\: Total}}{Survey_{Constraint \\: Total}}\n\\label{eq:ipf-word}\n\\end{equation}\n\n\\begin{equation}\nw(n+1)_{ij} = \\frac{w(n)_{ij} \\times sT_{i}}{mT(n)_{i}}\n\\label{eq:ipf}\n\\end{equation}\n\nwhere $w(n+1)_{ij}$ is the new weight for individuals with characteristics $i$\n(age, in this case), and $j$ (sex),  $w(n)_{ij}$ is the original\nweight for individuals with these characteristics, $sT_{i}$ is element\nmarginal total of the small area constraint, $s$\n(Table \\ref{t:s}) and $mT(n)_{i}$ is the marginal total of category\n$j$ of the aggregated results of the weighted\nmicrodata, $m$ (Table \\ref{t:m}).\n$n$ represents the constraint number.\n\nDo not worry about completely understanding the above procedure for now, its\nmeaning will probably take time to sink in.\nMore important is implementing it.\n\nFollow the blue emboldened values in tables 1 to 4 to fill in the\nquestion marks in \\cref{t:new-weights} and calculate the new weight\nof individual 3, a male under 50 years of age.\nTo validate your calculation, the sum of weights ($W_1$ to $W_5$)\nshould equal the population of the zone (12, in our example). This is a\nuseful check.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Reweighting the hypothetical microdataset in order to fit\nTable \\ref{t:s}.}\n\\begin{tabular}{lllll}\n\\toprule\n{Individual} & {Sex} & {age-group} & {Weight} &\n{New weight, w(2)} \\\\ \\midrule\n1 & Male & Over-50 & 1 & $1 \\times 4/3 = \\frac{4}{3}$ \\\\\n2 & Male & Over-50 & 1 & $1 \\times 4/3 = \\frac{4}{3}$ \\\\\n3 & Male & Under-50 & 1 & $1 \\times \\frac{\\textbf{\\color{blue}8}}{\\textbf{\\color{blue}?}} = \\textbf{\\color{blue}?}$ \\\\\n4 & Female & Over-50 & 1 & $1 \\times 4/3 = \\frac{4}{3}$ \\\\\n5 & Female & Under-50 & 1 & $1 \\times \\frac{8}{2} = 4$ \\\\\n\\midrule\nTotal & & & 5 & 12 \\\\\n\\bottomrule\n\\end{tabular}\n\\label{t:new-weights}\n\\end{table}\n\n\\subsection{Re-aggregation after the first constraint} \\label{sreag}\nAfter creating $Weight_{New}$ for all possibilities of the first constraint (under 50 and 50$+$ for age),\nthe individual level data is re-aggregated before the next weight can be calculated using the next constraint.\n\nRe-aggregating the individual-level data --- to compare the marginal totals with the constraint tables\nready for the second constraint --- will result in\n\\cref{t:m2}. To generate this table, the new weights ($w(2)$,\npresented in \\cref{t:new-weights}) are multiplied by the \nnumber of individuals in each category. Thus, to calculate the new\nestimate for the number of males we multiply the weight of\nmales under 50 (4 --- follow the emboldened values above) by the\nnumber of individuals in this category (1) and add the corresponding values\nfor males over 50 ($\\frac{4}{3}$ and 2). This is as follows\n(see the bold value in the bottom left of \\cref{t:m2}):\n\n\\begin{equation}\n\\sum_{i = male} m(2) \\; = \\; \\textbf{\\color{blue} 4} \\times \\textbf{\\color{blue}1} + \\frac{4}{3} \\times 2 \\; =  \\;\n\\textbf{\\color{blue}4} + \\frac{8}{3} \\; = \\textbf{\\color{blue}6} \\frac{\\textbf{\\color{blue}2}}{\\textbf{\\color{blue}3}}\n\\end{equation}\n\nAfter performing this process of re-aggregation (\\cref{t:m2}) for all categories\nin the **age** constraint,n\nthe next stage is to repeat \\cref{eq:ipf} for the **sex** constraint to generate a\nthird set of weights, by replacing\nthe $i$ in $sT_{i}$ and $mT(n)_{i}$ with $j$ and incrementing the value of n:\n\n\\begin{equation}\nw(3)_{ij} = \\frac{w(2)_{ij} \\times sT_{j}}{mT(2)_{j}}\n\\label{eq:ipf2}\n\\end{equation}\n\n\\subsection{Test your understanding: the second constraint}\nTo test your understanding of IPF, try to apply \\cref{eq:ipf2} to the\ninformation above\nand that presented in \\cref{t:m2}.\nThis should result in the following vector of new weights, for individuals 1 to\n5. Calculate the correct values and pencil them in in place of the question\nmarks.  % Add online link to answers - tinyurl\n\n\n\\begin{equation}\nw(3) = (\\frac{6 \\times \\frac{4}{3}}{6 \\frac{2}{3}},\\;\\;\\; \\frac{6 \\times\n\\frac{\\;4\\;}{\\;3\\;}}{\\;?\\;},\\;\\;\\; \\boldsymbol{ \\frac{\\color{blue}6 \\times 4}{\\color{blue} 6\\frac{2}{3}\n}}, \\;\\;\\;\\frac{\\;?\\; \\times \\;?\\;}{?}, \\;\\;\\;\\frac{4 \\times 6}{5 \\frac{1}{3}})\n% w(3) = (\\frac{6}{5}, \\frac{6}{5}, \\frac{18}{5}, \\frac{3}{2}, \\frac{9}{2})\n\\end{equation}\n\nAfter simplifying these fractions, the results are as follows.\nOne `sanity' check of your method here is whether the sum of these\nweights is still equal to the area's total population of twelve. Test this is\ntrue:\n\n\\begin{equation}\n% w(3) = (\\frac{6}{5}, \\frac{6}{?}, \\boldsymbol{ \\frac{4 \\times 6}{ 6\\frac{2}{3}\n% }}, \\frac{?}{?}, \\frac{9}{2})\nw(3) = (\\frac{6}{5}, \\frac{6}{5}, \\boldsymbol{\\frac{\\color{blue}18}{\\color{blue}5}}, \\frac{3}{2},\n\\frac{9}{2})\n\\label{finalout}\n\\end{equation}\n\nWhat do the weights in w(3) actually mean? They indicate how representative\neach individual is of the target zone after one iteration of IPF, constraining\nby age and sex. Individual number 5 has the highest weight because there is\nonly one young female in the survey dataset yet seemingly many in the area in\nquestion.\n\nNotice also that after each iteration the fit between the marginal\ntotals of $m$ and $s$\nimproves.\\footnote{This can be checked by comparing the aggregated weighted\nindividuals with the small area constraints. Total absolute error (TAE),\ndefined as the sum of all differences between simulated and observed marginal\ntotals, improves between $m(1)$ to $m(2)$, falling from\n14 to 6 in \\cref{t:m} and \\cref{t:m2} above. TAE for $m(3)$ (not shown,\nbut calculated by aggregating $w(3)$) improves even more, to 1.3.\nThis number would eventually converge to 0 through subsequent\niterations, a defining feature of IPF.}\n\n\n\\begin{table}[htbp]\n\\centering\n\\caption[Aggregated results after constraining for age]{The\naggregated results of the weighted\nmicrodata set after constraining for age ($m(2)$).\n}\n\n\\begin{tabular}{cllll}\\toprule\nMarginal totals&  & \\multicolumn{2}{c}{$i$} & \\\\\n& Age/sex & Male & Female & T\\\\ \\midrule\n\\multirow{2}{*}{$j$} & Under-50 & \\textbf{\\color{blue}4} & 4 & 8\\\\\n& Over-50 & $\\frac{8}{3}$ & $\\frac{4}{3}$ & 4 \\\\\n& T & $\\boldsymbol{{\\color{blue}6}\\frac{\\color{blue}2}{\\color{blue}3}}$ & 5$\\frac{1}{3}$ & 12\\\\\n\\bottomrule\n\\end{tabular}\n\\label{t:m2}\n\\end{table}\n\n% A key benefit from a policy perspective is that\n% IPF and other spatial microsimultion techniques\n% can provide estimation of variables whose values are not\n% known at the local level (e.g. income).\n\n\\subsection{IPF in a spreadsheet}\n% Computer code is absent from most people's daily lives so the\n% R code we use to automate IPF will likely seem like a foreign language\n% to many. Spreadsheet programs like Microsoft Excel and LibrOffice Calc are\n% comparatively well known, though.\nR is an unfamiliar programme to many, so before we move on to producing an R script that can calculate weights, we will carry out an intermediary step in a spreadsheet programme.\\footnote{This\nexample was produced by Phil Mike Jones --- see \\href{http://www.philmikejones.net/}{philmikejones.net}.}\nFor small examples like the one we are using, a spreadsheet has the two main advantages that: it is already familiar to many so can act as a bridge to calculating IPF using a computer; and it is easier to see and recall more than one table in a spreadsheet compared to R with R Studio.\n\nOpen sms-spreadsheet-exercise.xlxs if you are using Microsoft Excel or sms-spreadsheet-exercise.odt (note the different file types) if you are using LibreOffice/OpenOffice Calc and follow the tasks outlined in the file. The steps are exactly the same as the pen and paper example above to allow you to more easily check your answers; to help with the transition of performing the calculations of IPF in a computer programme; and to aid recall of the procedure through repetition.\n\n\\section{IPF in R} \\label{simplementing}\n% priority: automate!!!\nSo far we have implemented IPF by hand and in a spreadsheet.\nThis section explains how the IPF\n\\emph{algorithm} described above is implemented in R, using the\nsame input data. Now we will be working within the RStudio\nenvironment with the `smsim-course' project \nloaded and the `simple.R' file open in the top left panel\n(\\cref{frsimple}). \n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=15cm]{rs-simple}\n\\end{center}\n\\caption{The RStudio user interface with the `simple.R' file loaded in\nthe top left window and some output displayed in the bottom left window.}\n\\label{frsimple}\n\\end{figure}\n\nNote a couple of things from this figure:\nthere are many explanations of the code and these are commented out\nby the \\texttt{\\#} ('hash') symbol. Also not in the bottom left panel\nR displays the output of the commands we send it from the script.\nHitting the \\texttt{Ctrl} and \\texttt{Enter} keys simultaneously from\nwithin the upper (script) window will cause R to run these commands,\nline by line. In the code blocks below, any outputs from R are preceded\nby `\\texttt{\\#\\#}'. Comparing these outputs with the output on your screen will\nensure that R is behaving as it should.\n\nWe will spend the rest of this section working through\nthe commands contained in this file.\\footnote{A longer\ntutorial is available from Rpubs, a site dedicated\nto publishing R analyses that are reproducible:\n\\href{http://rpubs.com/RobinLovelace/5089}{rpubs.com/RobinLovelace/5089}.}\n% This section is based on ``Spatial microsimulation in R: a\n% beginner’s guide to iterative proportional fitting (IPF)'', a tutorial\n% written to accompany a methods paper on integerisation\nThe code snippets are taken from the `simple.R' script file (just a plain text\ndocument, like all R scripts) so you can copy and paste. However, it is\nhighly recommended to not look at this file until you need to, towards\nthe end of this\nsection. This is learning by typing, as advocated by \\citet{shaw2013learn}.\n\n\\subsection{Loading and exploring the input data}\n\nThe data presented in the above worked example are saved in the\n`simple' sub-folder of `data' as .csv files. To load them in R,\nwe use the following commands:\n\n\\begin{lstlisting}[float=h, caption={Loading the input data in R}, label=cusd]\nind <- read.csv(\"data/simple/ind.csv\") # load the individual level data\ncons <- read.csv(\"data/simple/cons.csv\") # load aggregate constraints\n\\end{lstlisting}\n\nTo check that the \\emph{data frames} have loaded correctly, simply\nask R to print their contents by entering the object's name. To display the\nindividual level data, for example, simply type \\verb/ind/:\\footnote{An alternative\nsolution is to click the object's name in `Environment' in\nRStudio's top right window.}\n\n\\begin{lstlisting}[float=h, caption={Checking the contents of the individual\nlevel data frame}, label=cout]\nind  # Show the data frame in R\n##  id age sex\n##   1  59   m\n##   2  54   m\n##   3  35   m\n##   4  73   f\n##   5  49   f\n\\end{lstlisting}\n\nNote that the input data is identical to the tables illustrated above for\nconsistency. Instead of constraining for just one zone as we did in the example\nabove, we will fit the data to six zones here. To subset a dataset, add square\nbrackets to the end of the object's name in R. Entering \\verb cons[1:2, ] ,\nfor example, should output the first two rows of the constraint variable.\nWithin the square brackets the number before the comma refers to rows (blank\nmeans all rows) and the numbers after the comma refer to columns. If the output\nof this command looks like the text below, congratulations are in order:\nyou have successfully loaded a subset of the constraint dataset.\n\n\\begin{lstlisting}[float=h, caption={Printing a subset of the constraint data\nin R},\nlabel=cou]\ncons[1:2, ]\n##    X16.49 X50. m f\n##  1      8    4 6 6\n##  2      2    8 4 6\n\\end{lstlisting}\n\nNote that the top row is identical to \\cref{t:s} --- we can therefore compare\nthe results of doing IPF on the computer with the results obtained by hand.\n\n% !!! Manual input of data for book!!!\n% Usually the input data for spatial microsimulation are loaded from\n%  .csv files, one for each\n% constraint and one for the input microdata. These are read-in with the command\n% \\verb read.csv . For the purposes of understanding how the model works,\n% the dataset is read line by line, following the\n% example above. The following code creates example datasets,\n% based on the same hypothetical survey of 5 individuals described above,\n% and 5 small areas. The spatial microsimulation model will select individuals\n% based on age and sex and mode of transport (mode of transport\n% is also used on the larger online example described in footnote \\ref{fnrpub}).\n% For consistency with the (larger) model used for the paper, \n% the individual level data will be referred to as USd (Understanding Society dataset)\n% and the geographic data as all.msim (for all constraint variables).\n% The code to read-in the individual level data are presented in code sample \\ref{cusd}.\n% When called, the data are then displayed as a table (see listing \\ref{cout}).\n% \\begin{lstlisting}[float=h, caption={Manual input of individual level data\n% in R}, label=cusd]\n% # Read in the data in long form (normaly read.table() used)\n% c.names <- c(\"id\", \"age\", \"sex\")\n% USd <- c(       1, 59, \"m\",\n%                 2, 54, \"m\",\n%                 3, 35, \"m\",\n%                 4, 73, \"f\",\n%                 5, 49, \"f\")\n% USd <- matrix(USd, nrow = 5, byrow = T) # Long data into matrix\n% USd <- data.frame(USd) # Convert this into a dataframe\n% names(USd) <- c.names # Add correct column names\n% USd$age <- as.numeric(levels(USd$age)[USd$age]) # Age is a numeric\n% \\end{lstlisting}\n% \\begin{lstlisting}[float=h, caption={Output of the USd data frame}, label=cout]\n% USd # Show the data frame in R\n% ##   id age sex\n% ## 1  1  59   m\n% ## 2  2  54   m\n% ## 3  3  35   m\n% ## 4  4  73   f\n% ## 5  5  49   f\n% \\end{lstlisting}\n% The same procedure applies to the geographical data (listing \\ref{cgeo}).\n% \\begin{lstlisting}[float=h*, caption={Geographic data input}, label=cgeo]\n%  category.labels <- c(\"16-49\", \"50+\" # Age constraint\n%              ,\"m\", \"f\" # Sex constraint\n%              # more constraints could go here\n%              )\n% all.msim <- c(  8, 4,    6, 6,   # Original aggregate data\n%                 2, 8,    4, 6,   # Elderly\n%                 7, 4,    3, 8,   # Female dominated\n%                 5, 4,    7, 2,   # Male dominated\n%                 7, 3,    6, 4    # Young\n%                 )\n% all.msim <- matrix(all.msim, nrow = 5, byrow = T) \n% all.msim <- data.frame(all.msim) # Convert to dataframe\n% names(all.msim) <- category.labels # Add correct column names\n% \\end{lstlisting}\n\n% IPF relies on the assumption that all constraint variables will contain the\n% same number of people. This is logical (how can there be more people classified\n% by age than by sex?) but can cause problems for constraint variables that use\n% only a subset of the total population, such as those who responded to questions on\n% travel to work. To overcome this problem, it is possible to normalise the\n% constraint variables, setting the total for each to the one that has the most\n% reliable total population. This worked example simply checks whether\n% or not they are (listing \\ref{ccheck}).\n\n% \\begin{lstlisting}[float=h, caption={R code to check the constrain populations\n% match}, label=ccheck]\n%  # Check totals for each constraint match\n% rowSums(all.msim[,1:2]) # Age constraint\n% ## [1] 12 10 11  9 10\n% rowSums(all.msim[,3:4]) # Sex constraint\n% ## [1] 12 10 11  9 10\n% \n% rowSums(all.msim[,1:2]) == rowSums(all.msim[,3:4])\n% ## [1] TRUE TRUE TRUE TRUE TRUE\n% \\end{lstlisting}\n\nUsing the subset command just introduced, we must now subset the constraint (\\verb cons ) data frame so we can access each constraint (age or sex) separately. Using your knowledge of the subsetting command, we select columns 1 to 2, and 3 to 4 leaving all rows in place. Give these subsets (constraints) the name \\verb con1  and \\verb con2  respectively:\n\n\\begin{lstlisting}[float=h, caption={Subsetting the constraint file}]\ncon1 <- cons[ ,1:2]\ncon2 <- cons[ ,3:4]\n\\end{lstlisting}\n\n\nTo make the individual level dataset comparable to the aggregate level we must\nfirst categorise it and count the number in each category. This job is\nperformed by the R \\emph{script} categorise.R, contained in the `simple' data\nfolder. Run it by entering\n\\verb source(\"data/simple/categorise.R\")) .\\footnote{Note\nthat the\n`categories.R' script must be modified each time it is used with a different\ndataset.}\nThis results in the creation of a\nnew object called \\texttt{ind.cat} in R, which is a matrix of 0s and\n1s with same number of columns as the constraint data --- try\nexploring \\texttt{ind.cat} using the square bracket notation mentioned above.\n\nTo check that the script has worked\nproperly, lets count the number of individuals it contains:\n\n\\begin{lstlisting}[float=h, caption={Output from ind.cat}]\ncolSums(ind.cat)\n##    a16.49   a50.      m      f\n##         2      3      3      2 \n\\end{lstlisting}\n\nThe sum of both age and sex variables is 5 (the total number of individuals): it\nworked! Now the data is in the same form as the constraint variables and we\nhave checked the data makes sense, we can create the `weight' matrix and begin\nreweighting the individuals to zones (\\cref{lscreate}).\n\n\\begin{lstlisting}[float=h, caption={Creating the weight and aggregated\nindividual level matrices --- see lines 21 and 22 in `simple.R'}, label=lscreate]\nweights <- array(1, dim=c(nrow(ind),nrow(cons))) \nind.agg <- matrix(rep(colSums(ind.cat), nrow(cons)), nrow(cons) )\n\\end{lstlisting}\n\nThe code in \\cref{lscreate} may not make total sense. Do not worry about this for now, you can always\ncheck what is going on later by checking R's documentation (e.g. try entering\n\\verb ?rep ) or\nby pressing the tab key when entering arguments for R commands in RStudio. For the time\nbeing, it is best to press-on with the example and understand the concepts ---\nthe details can be looked at later.\n\n\\subsection{Reweighting by the age constraint}\n\nWith all of these objects in place, we are ready to begin allocating new weights\nto the individuals via IPF. Below is written in code the IPF formula presented\nin \\cref{ipfeq}. It begins with a couple of \\emph{for loops}, one to iterate through\neach zone (hence the \\texttt{1:nrow(cons)} argument, which means ``from\n1 to the number of zones in the constraint data'') and one to iterate\nthrough each category within constraint 1 (under 50 and over 50 in this case).\nBefore running this code, ensure that all previous commands in the `simple.R'\nscript have been performed.\n\n\\begin{lstlisting}[float=h, caption={Calculating new weights for the individuals\nbased on constraint 1 (age) via IPF --- see line 30 and beyond in `simple.R'}]\nfor (j in 1:nrow(cons)){\n  for(i in 1:ncol(con1)){\n weights[which(ind.cat[,i] == 1),j] <- con1[j,i] / ind.agg[j,i]}}\n\\end{lstlisting}\n\nThe above code updates the weight matrix by dividing\nthe census constraint (\\texttt{con1}) by the aggregated version\nof the individual level data, just as in \\cref{ipfeq} above.\nFurther explanation is provided in \\cref{sipfcake}: for now let's push on\nto the next block of code, which updates the aggregated data (named\n\\texttt{ind.agg} in R) based on the the new weights.\n\n\\subsection{Re-aggregation}\n\n\\begin{lstlisting}[float=h, caption={Re-aggregation of the individual\nlevel data based on the new weights (see lines 35 and 36 in `simple.R')}]\nfor (i in 1:nrow(cons)){ # convert con1 weights back into aggregates\n  ind.agg[i,]   <- colSums(ind.cat * weights[,i])}\n\\end{lstlisting}\n\nThe above line of code multiply the new weights by the\nflat version of the individual-level data, one zone (i) at a\ntime. This is equivalent to the re-aggregation stage described in\n\\cref{sreag} and the result can be checked by printing the first\nrow of our new \\texttt{ind.agg} object:\n\n\\begin{lstlisting}[float=h, caption={The new aggregate (marginal) totals for each category\nafter weights have been constrained by age. Compare with \\cref{t:m2} above.}]\nind.agg[1, ]\n##    [1] 8.000000 4.000000 6.666667 5.333333\n\\end{lstlisting}\n\nTo recap, we have updated `ind.agg' with new values\nthat are constrained by the age variable. This seems to improve the fit\nbetween the census and individual-level data as the marginal totals for\nage are now equal to the aggregate values for age of zone 1.\nThis is further\nverified in the `simple.R'\nscript file with a line of code that calculates the Total Absolute Error\nafter each iteration --- \\verb sum(abs(ind.agg - cons)) .\nCheck that the error decreases with each iteration (see \\cref{svalid} for\nmore on model checking and validation).\n\n% At this stage you should switch to running the code in `simple.R'. In RStudio\n% to run a line of code from the top left window, simply press \\verb Ctl-Enter .\nContinue to enter the code contained in `simple.R' line by line to constrain\nby the second constraint, sex.\nNotice that we have done in code what was previously done by\nhand and in a spreadsheet. % TODO: and in a spreadsheet!\nDepending on which method you feel most comfortable with, it is worth\nspending time repeating the previous steps to ensure they make sense.\n\nThe other advantage of running the same process 3 times in 3 different ways\nis that we can cross-compare the results from each.\nCompare\nthe following listing with \\cref{finalout}, for example: are the weights the same?\n\n\\begin{lstlisting}[float=h, caption={W(3) after constraining by age and sex in R. \nCompare with \\cref{finalout}}]\nweights3[,1] # check the weights allocated for zone 1\n##    [1] 1.2 1.2 3.6 1.5 4.5\n\\end{lstlisting}\n\n% \\newpage\n\\subsection{Iterations}\n\nIn the above example, we have seen the `bare bones' of spatial microsimulation\nusing IPF to generate weights from which sample populations can be created.\nTo perform multiple iterations of the same model, we have prepared a slightly\nmore complex script than `simple.R' called `simple-iterations.R' that can \nalso be found in the project's root directory. We set the number\nof iterations manually (just\nchange the value of \\verb num.its ). This happens by calling another\nscript called `e2.R' which runs the re-weighting process\nrepeatedly, once for each iteration. Notice that only 2 full\niterations, the perfect solution is reached (r = 1).\n\nThere is great scope for taking the analysis further:\nsome further tests and plots are presented on the on-line\nversions of this section. The simplest case is contained in\nRpubs document \\href{http://rpubs.com/RobinLovelace/6193}{rpubs.com/RobinLovelace/6193} \nand a more complex case (with three constraints) can be found in Rpubs document\n\\href{http://rpubs.com/RobinLovelace/5089}{5089}. For now, however,\nwe progress to a more complex example, CakeMap.\n\n\\section{CakeMap: A worked example}\n\nIn this section we will process real data to arrive at an important result:\nan estimate of the amount of cake eaten in different wards in West Yorkshire. The\nexample is deliberately rather absurd: hopefully this will make\nthe technique more memorable. An attempt has been made to present the method\nin a generalisable was as possible, allowing users to apply the technique described\nhere to their own data.\n\nThe code needed to run the main part of the example is contained within `cMap.R'.\nNote that this script makes frequent reference to files contained in the `cakeMap'\nfolder `data'. \\verb read.csv(\"data/cakeMap/ind.csv\") , for example, is used to read\nthe individual level data into R.\n\nBecause this example uses real world data (albeit in anonymised form),\nwe will see how important the process of `data cleaning' is.\nRarely are datasets downloaded in exactly the right form for them to\nbe pumped directly into a spatial microsimulation model. For that reason\nwe describe the process of loading and preparing the input data first:\nsimilar steps will be needed to use your own data as an input into a\nspatial microsimulation model.\n\n\\subsection{Loading and preparing the input data}\nThe raw constraint variables for CakeMap were downloaded from\nthe Infuse website (\\href{http://infuse.mimas.ac.uk/}{infuse.mimas.ac.uk/}).\nThese, logically enough, are stored in the `cakeMap/data/' directory\nas .csv files and contain the word `raw' so it is clear which files\ncontain the raw data. The file `age-sex-raw.csv', for example is the raw\nage and sex data that was downloaded. As the screenshot in \\cref{fraw} shows,\nthese datasets need to be\nprocessed before they can used as an input\nin our model. `con1.csv' stands for\n`constraint 1' and represents this age/sex data after it has been processed.\n\n\\begin{figure}\n \\includegraphics[width=12cm]{raw-data-screenshot}\n\\centering\n\\caption{The raw input data for the CakeMap model, downloaded from the InFuse website.}\n\\label{fraw}\n\\end{figure}\n\nTo ensure reproducibility in the process of converting the raw data into\nthis `spatial microsimulation ready' dataset, all of the steps used to\nclean and rearrange the data have been saved. Take a look at the R script file\n`process-age.R' --- these are the kinds of steps that the researcher will need to\nundertake before performing a spatial microsimulation model. \nHere is not the place to delve into the details of data reformatting;\nthere are resources dedicated to that \\citep{tidy-data, kabacoff2011r}.\nHowever, it is worth taking a look at the `process-age.R' script and the\nother `process*' files to see how R can be used to quickly\nprocess complex raw data into a form that is\nready for use in spatial microsimulation.\n\nThe input data generated through this process of data preparation are named\n`con1.csv' to `con3.csv'. For simplicity, all these were merged into a single\ndataset called `cons.csv'. All the input data for this section\nare loaded with the following commands:\n\n\\begin{lstlisting}[float=h, caption={Loading the input data for CakeMap (see `cMap.R')}]\nind <- read.csv(\"data/cakeMap/ind.csv\")\ncons <- read.csv(\"data/cakeMap/cons.csv\")\n\\end{lstlisting}\n\nTake a look at these input data using the techniques learned in the previous\nsection. To test your understanding, try to answer the following questions:\nwhat are the constraint variables? How many individuals are in the survey microdataset?\nHow many zones will we generate spatial microdata for?\n\nFor bonus points that will test your R skills as well as your practical knowledge\nof spatial microsimulation, try constructing queries in R that will automatically\nanswer these questions.\n\nIt is vital to understand the input datasets before trying to model them, so\ntake some time exploring the input. Only when satisfied with your understanding of\nthe datasets (a pen and paper can help here, as well as R!) is it time\nto move on to generate the spatial microdata using IPF.\n\n\\subsection{Performing IPF on the CakeMap data}\n\\label{sipfcake}\nThe R script used to perform IPF on the CakeMap data is almost identical to\nthat used to perform the process on the simple example in the previous section.\nThe main difference is that there are more constraints (3 not 2) but otherwise\nthe code is very similar.  Confirm this for yourself: it will be instructive\nto compare the `simple.R' and\n`\\href{https://github.com/Robinlovelace/smsim-course/blob/master/cMap.R}{cMap.R}'\nscript files. Note that\nthe former works on a tiny example of 5 input individuals whereas the latter\ncalculates weights for 916 individuals. The code is generalisable, meaning\nthat the IPF process would still work given slightly different inputs.\nThe script would still work if more individuals or zones were added although\nthe code will need to be adjusted to accept differenct constraints.\nTo ensure the process works with different\nconstraints, the file `categorise.R' must also be.\n\nNote that `cMap.R' contains 100 lines --- longer than the `simple.R' file.\nThe additional length is due\nto the third constraint, additional iterations (see line 66 onwards) and some\npreliminary analysis. The best way to understand what is happening in this\nscript is simply to read through it and run chunks of it in R (remember the \n\\verb Ctl-Enter ~ command) to see what happens. Try to `think like a computer'\nand imagine what each line of code is doing. This is inevitably difficult\nat first so it is recommended that users add more comments to lines\nthat cause confusion to help remember what is going on. To help this process,\nthe meaning of some of the trickier and more important lines is explained below.\nOne potentially confusing area is the representation of the \\texttt{weights} object,\nused to store the weights after each constraint. To understand what is happening\ntry entering this code into R and seeing what happens:\n\n\\begin{lstlisting}[float=h, caption={Creating the initial weight matrix --- see lines\n28 to 30 in cMap.R}]\n# create weights in 3D matrix (individuals, areas, iteration)\nweights <- array(dim=c(nrow(ind),nrow(cons),num.cons+1))\nweights[,,num.cons+1][] <- 1 # sets initial weights to 1\nini.ws <- weights[,,num.cons+1]\n\\end{lstlisting}\n\nIn the above code, \\texttt{weights} is created by the \\texttt{array}\ncommand, which in fact creates many 2 dimensional matrices in a\n`data cube'. This is different from the weight object used in the\n`simple.R' script, which had only 2 dimensions.\n\nIf this is confusing, just remember that\nthe `square' weight matrix --- where each column represents a zone\nand each row represents an individual --- can be returned to by\nspecifying the third dimension. Dimensions in R are specified by\nthe square brackets, so the initial weights can be called by typing\n\\texttt{weights[,,4]}. This should result in hundreds of 1s, the initial weights.\nTo print a more manageable-sized output, remember how to subset the other\ndimensions: \\texttt{weights[1:3,1:5,4]}, for example, will print the first\n3 rows (individuals) and first 5 columns of the 4th matrix. Why do we do it like this?\nSo that we only need one weights object to remember all the previous weights\nso we can go back and check whether the model fit is improving from one constraint\nand one iteration to the next.\n\nNow, when the new weights are set after iteration 1, they are saved as follows,\nwithin a nested \\emph{for loop} that iterates over all zones (j) and\nconstraint categories (i):\n\n\\begin{lstlisting}[float=h, caption={Saving the new weights after the first\nconstraint of IPF --- see line 43 in `cMap.R'}, label=cmapipf]\nweights[which(ind.cat[,i] == 1),j,1] <- con1[j,i] /ind.agg[j,i,1]}}\n\\end{lstlisting}\n\nNote the heavy use of square brackets in \\cref{cmapipf}.\nLet's break down each element of the code within these square `subsetting' brackets:\n\\begin{itemize}\n \\item \\texttt{which(ind.cat[,i] == 1)} is a subset of the rows in the weight matrix\n which correspond to all individuals who belong to category i. (Remember,\n \\texttt{ind.cat} is the `wide' version of the individual level data, containing\n 1s for all categories to which each individual belongs.)\n \\item \\texttt{,j,1]} specify the column and constraint respectively.\n Because we are in a for loop, j simply runs through all categories within the\n current constraint (age/sex). The 1 is selected because this is the first set of\n re-weighting (this is analogous to say $w(2)$ --- the first new weight ---\n in the IPF equations in the previous section).\n \\item Finally, \\texttt{con1[j,i] /ind.agg[j,i,1]} is R's implementation of\n \\cref{eq:ipf}: we divide the desired census values by the current simulated\n totals for each category based on the individual's current weight.\n\\end{itemize}\n\nThis explanation should help understand what is going on in other\nparts of the script file also --- for example try printing subsets of\nthe \\texttt{weights} object after running constraint 1, 2 and then 3 to\nsee how the weights change.\n\n% \\subsection{Processing the output}\n\n\\subsection{Integerisation}\nIntegerisation increases the utility of results generated\nby IPF, such as the weights produced in the cakeMap example.\nMany of the most useful things we can do with individual-level data can only\nbe done with \\emph{whole individuals}. The weight matrices we have generated, however,\nare \\emph{fractional weights}, which could imply, for example that 0.437 of an\nindividual resides in a zone. From the perspective of an agent based (but not\naggregated level) model, this is clearly absurd.\n\nHere is not the place to delve into the various methods available for integerisation ---\nsee \\citep{Lovelace2013-trs} for detail on the problem. Suffice to say that\nof the two broad approaches available, deterministic and probabilistic, the latter\nis found to generate more accurate results.\n\nTwo scripts to \\emph{integerise} the final weights (referred to as \\texttt{fw}\nin the code) of the cakeMap model\nare provided: `pp-integerise.R' and `TRS-integerise.R', which stand for\n`proportional probabilities' and `truncate, replicate, sampe'.\nThese reside in the `data/cakeMap' folder.\nWe will not describe the theory underlying the method here (see \\citealp{Lovelace2013-trs} for more).\nInstead, we will describe a few of the functions contained in these scripts that are\nthe keys to understand how they work. Both scripts begin by creating\na series of objects that are used during the integerisation process\nand to save the results \\cref{intobjects}.  \n\n\\begin{lstlisting}[float=h, caption={The new objects that\nare created in at the beginning of the integerisation scrips (see lines 7 to 8 in `pp-integerise.R' and\n`TRS-integeris.R').}, label=intobjects]\nintall <- ints <- as.list(1:nrow(cons)) \nintagg <- cons * 0\n\\end{lstlisting}\n\nIn the first line of \\cref{intobjects}, two identical objects are created\n(note the double use of the \\texttt{<-} object assignment symbol).\nThese are set objects in the list class, allowing each element to be\nallocated as any other object. This differs from the stricter\n`data.frame' and `matrix' or `array' objects we have been using so far.\nSubsequently in the script, individual elements are referred to using\ndouble square brackets. For example, see \\texttt{ints[[i]] <-} on line\n15 of `pp-integerise.R'. Using list elements provides flexibility:\nwe can set any list object to be anything we like without\nworrying about constraints imposed by its class or dimensions.\n\nThe key function in the proportional probabilities\nscript is \\texttt{sample}, used in a for loop over\nall areas (\\cref{pp-sample}).\n\n\\begin{lstlisting}[float=h, caption={The sample function,\nkey to R's implementation of the proportional probabilities\nintegerisation algorithm (see lines 15 and 16 of `pp-integerise.R').}, label=pp-sample]\nints[[i]] <- sample(which(fw[,i] > 0), size = sum(con1[i,]), \n                      prob=fw[,i], replace = T) \n\\end{lstlisting}\n\nThe \\texttt{sample} function in \\cref{pp-sample} contains four arguments, and\nthe meaning of each is described below:\n\\begin{itemize}\n \\item The first item (\\texttt{which(fw[,i] > 0)}) provides the \\emph{vector} of\nnumbers or characters from which the sample will be drawn. In this case we are referring\nto the weights of all individuals for zone i (all weights should be above 0).\n\\item The size argument tells R how many items from the object should be selected.\nIn this case it is simple: the total population of zone i.\n\\item The cleverest thing about R's sampling function is that it allows probabilities\nof being selected to be assigned to each element. Here we are setting the probability\nof selection as proportional to the final weight. (Clearly the probability of\nselection can never be greater than 1, as is the case with many weights; R\nautomatically normalises the values.)\n\\item The final argument tells R that we want to allow repeat sampling:\nthe same individual can be selected more than once. This makes sense in a\ndataset where wheights can get very large: an individual with a weight of 10.4,\nfor example, should ideally be selected between 10 and 11 times.\n\\end{itemize}\n\nNote that `trs-integerise.R' also uses the sample function, in this case to\n`top up' individuals already selected from the `truncate, replicate' stages\nof the `truncate, replicate, sample' method \\citep{Lovelace2013-trs}.\nFor more information on implementing TRS in R, refer to this paper and the paper's detailed\n\\href{http://eprints.whiterose.ac.uk/77037/7/lovelacesupplement-3.pdf}{supplementary information},\nwhich provides reproducible results to suggest using TRS for accuracy.\n(As an aside, the accuracy of each integerisation algorithm was\ntested on the cakeMap data --- showing TRS to yield better results for this application.\nThe results can be reproduced by running the `integerisation-test.R'\nscript, \\href{https://github.com/Robinlovelace/smsim-course/tree/master/vignettes}{located in the `vignettes' folder}.)\n\nThe most useful result of these integerisation algorithms is the object \\texttt{intall}.\nThis is list of data frames containing, each containing a population equal\nto the population of its respective zone. Each has the same\nvariables as are present in the original survey dataset. Make\nthis object easier to analyse, a series of commands is provided\n(but commented out) at the end of the script to convert the list into\na more manageable data.frame object. Try entering\n\\texttt{nrow(intall.df)} after running this code (remove the comments first!)\nWe have cloned hundreds of thousands of individuals.\nClearly, all this data takes up more RAM on the computer, as can be seen by\nasking \\texttt{object.size(intall.df)}. Try comparing this result with the\nsize of the original survey dataset `ind'. From this point onwards,\nwith the individual-level data in a straightforward dataframe\n(with the zone number kept in the `zone' column), the modelling and\nanalysis applications should be comparatively straightforward.\n\nNext we move on to a vital consideration in\nspatial microsimulation models such as cakeMap: validation.\n\n\\subsection{Model checking and validation}\n\\label{svalid}\nTo make an analogy with food safety standards, openness about mistakes is\nconducive to high standards \\citep{Powell2011}. Transparency in model\nverification is desirable for similar reasons. The two main strategies are 1) \ncomparing the model results with knowledge of how it \\emph{should}\nperform \\emph{a-priori} (model checking) and 2) comparison between the model\nresults and empirical data (validation).\n\nWithin the two R scripts we have been using for reweighting the individual-level\ndata so far (`simple.R' and `cMap.R'),\nthere are \\emph{already} commands whose function is to check the\nresults are as expected. Returning to the former, note that on line 24\nof `simple.R', the fit between the survey data and census data is\ntested with \\texttt{sum(abs(ind.agg - cons)) \\# the total absolute error}.\nThis, as the comment implies, calculates the total absolute error\n(TAE), which is a good measure of the fit between two aggregate-level\ndatasets and which is defined by the following formula:\n\n\\begin{equation}\n TAE = \\sum\\limits_{ij}|U_{ij} - T_{ij}|\n\\end{equation}\n\nStandardised Total Error is a related measure: $SAE = TAE/P$\nwhere $P$ is the total population of the study area.\nThus TAE is sensitive to the number of people\nwithin the model. SAE is not --- see more on model checking and tests of\nfin in \\citet{Lovelace2013-trs}\n\nNote that TAE is calculated twice more in `simple.R', after each\nadditional constraint has been applied. Work through the code slowly and\ncheck how TAE reduces after each constraint and each iteration.\n\nBeyond typos or simple conceptual errors in model code, more fundamental\nquestions should be asked of spatial microsimulation models. The validity\nof the assumptions on which they are built, and the confidence one should have\nin the results are important. For this we need external datasets.\nValidation is therefore a tricky topic --- see \\citet{Edwards2009} for\nmore on this and \\cref{sanalysis} for (an albeit unreliable) comparison\nbetween estimated cake consumption and external income estimates.\n\n\\subsection{Visualising the results}\nVisualisation is an important part of communicating quantitative\ndata, especially so when the datasets are large and complex so not\nconducive to description with tables or words.\n\nBecause we have generated spatial data, it is useful to create \na map of the results, to see how it varies from place to place.\nThe code used to do this found in `cMapPlot.R'. A vital function within\nthis script is `merge', which is used to add the simulated cake data\nto the geographic \ndata frame:\\footnote{`join'\nis an alternative to merge from the `plyr' package also used\nin the `cMapPlot.R' script that performs the\nsame task. Assuming `plyr' is loaded --- `library(plyr)' you can read\nmore about join by entering `?join' in R.}\n\n\\begin{lstlisting}[float=h, caption={The merge function for joining\nthe spatial microsimulation results with geographic data. \nCompare with \\cref{finalout}}]\nmerge(wardsF, wards@data, by = \"id\")\n\\end{lstlisting}\n\nThe above line of code by default selects all the data\ncontained in the first object (`wardsF') and adds to it new variables\nfrom the second object based on the linking variable (in this case ``id'').\nAlso in that script file you will encounter the function \\verb fortify ,\nthe purpose of which is to convert the spatial data object into a\ndata frame. More on this process is described in \\citet{lovelace2014introduction}.\nThe final map result of `cakeMapPlot.R' is illustrated \\cref{fcmap}.\n\n\\begin{figure}[h]\n \\centering\n\\includegraphics[width=11cm]{cakeMap}\n\\caption{Choropleth map of the spatial distribution of average frequency of\ncake consumption in West Yorkshire, based on simulated data.} \n\\label{fcmap}\n\\end{figure}\n\n\\subsection{Analysis} \\label{sanalysis}\nOnce a spatial microdataset has been generated that we are happy with,\nwe will probably want to analyse it further. This means exploring it --- its main features,\nvariability and links with other datasets. To illustrate this process we will\nload an additional dataset and compare it with the\nestimates of cake consumption per person  generated in the previous\nsection at the ward level.  \n\nThe hypothesis we would like to test is that cake consumption is linked to deprivation:\nMore deprived people will eat unheathily and cake is a relatively cheap `comfort food'.\nAssuming our simulated data is correct ---  a questionable assumption but lets roll with\nit for now --- we can explore this at the ward level thanks to a \n\\href{http://www.neighbourhood.statistics.gov.uk/dissemination/instanceSelection.do?JSAllowed=true&Function=&%24ph=61&CurrentPageId=61&step=2&datasetFamilyId=266&instanceSelection=121427&Next.x=22&Next.y=13&nsjs=true&nsck=false&nssvg=false&nswid=1920}{dataset on \nmodelled income from neighbourhood statistics}.\n\nBecause the income dataset was produced for old ward boundaries (they were slightly\nmodified for the 2011 census), we cannot merge with the spatial dataset based on the\nnew zone codes. Instead we rely on the name of the wards. The code below provides a\nsnapshot of these names and demonstrates how they can be joined using the `join' function.\n\n\\begin{lstlisting}[float=h, caption={The merge function for joining\nthe spatial microsimulation results with geographic data. \nCompare with \\cref{finalout}}]\nwards@data <- join(wards@data, imd)\nsummary(imd$NAME %in% wards$NAME)\n##       Mode   FALSE    TRUE    NA's \n##    logical      55      71       0 \n\\end{lstlisting}\n\nThe above code first joins the two datasets together and then checks the result by\nseeing how many matches names there are. In practice the fit between old names and new\nnames is quite poor: only 71 out of 124. In a proper analysis we would have to solve this\nproblem (e.g. via the command \\verb pmatch ~ which stands for partial match).\nFor the purposes of this exercise we will simply plot income against\nsimulated cake consumption to gain a feeling what\nit tells us about the relationship between cake consumption and wealth (\\cref{incomeCake}).\n\n\\begin{figure}\n\\centering\n \\includegraphics[width=7cm]{incomeCake}\n\\caption{Relationship between modelled average ward income and\n simulated number of cakes eaten per person per week.}\n\\label{incomeCake}\n\\end{figure}\n\nThe question raised by this finding is: why? \nNot why is cake consumption higher in wealthy areas (this has not\nbeen established) but: why has the model resulted in this correlation?\nTo explore this question we need to go back and look at the individual\nlevel data. The most relevant constraint variable for income was class.\nWhen we look at the relationship between social class and cake consumption\nin the Dental Health Survey, we find that there is indeed a link:\nindividuals in the highest three classes (1.1, 1.2, 2) have an average\ncake intake of 3.9 cakes per week whereas the three lowest classes have\nan average intake of 3.7. This is a relatively modest difference but,\nwhen averaging over large areas, it helps explain the result.\nThe class dependence of cake consumption in the Dental Health Survey is\nillustrated in \\cref{hm}.\n\n\\begin{figure}\n\\centering\n \\includegraphics[width=8cm]{hm}\n\\caption{Relationship between social class and cake consumption in the\nindividual level data.}\n\\label{hm}\n\\end{figure}\n\n\\section{Discussion}\n\nWhat have we learned during the course of this practical tutorial?\nGiven that it's geared towards beginners, from one perspective\none could say `not a lot' relative to the many potential applications.\nThe emphasis on understanding of the basics was deliberate: it is only\non strong foundations that large and long-lived buildings can be \nconstructed and the same applies to geographical research.\n\nThe focus on foundations is also due to the nature of spatial microsimulation\nresearch: few people are actively involved in applied work, let alone the\ndevelopment of new software and methods.\nSpecialist skills and understanding are needed to join this research area\nyet the barriers to entry are high.\nA central outcome of this course has been to provide an accessible `way in'\nto this exciting research area. Specifically, we have learned how to:\n\\begin{itemize}\n \\item Perform iterative proportional fitting, one of the commonly used\ntechniques for spatial microsimulation, by hand, in a spreadsheet and in R.\n \\item Iterate the process any number of times using pre-made R code.\n \\item Use simple queries and visuals to check the process has worked as expected.\n \\item Process the spatial microdata to create estimates of target variables and\njoin this with output with geographics zones.\n \\item Map the result and begin to look for explanation in the dataset.\n\\end{itemize}\n\nAn important lesson from the cakeMap example is\nthat spatial microsimulation can be used in careless and misleading ways.\nThe method is powerful, but it has dangers and limitations that must be ackowledged.\nMany articles on spatial microsimulation omit technical details of the method,\nlet alone code that would allow the findings to be reproduced. Great\ncare must be taken not to mislead.\n\nReturning to cakeMap, we have \\emph{not}\ndemonstrated cake consumption to be higher in wealthy areas of Leeds.\nWe have done nothing of the sort! We \\emph{have} created spatial microdata\nfor Leeds based on 3 constraint variables.\nThis spatial microdataset is not `real'. We can have a high level of confidence that the\nindividual attributes constrained by census data are roughly representative of the region.\nYet the interrelationship between these constraint variables\n(e.g.~the cross tabulation between age and car ownership) will tend towards the average\ncontained within the survey microdataset and is likely to mask regional variability. In an ward\nwhere the proportion of young drivers is very high, for example, our model result is likely\nto provide underestimates of the numbers of young people owning cars.\n\nThe estimates generated for the target variable --- cake consumption --- are likely to be\nfar from reality. They reflect the relationship between cake consumption and\na selection of constraint variables at the national level. Do we have good evidence to suggest\nthat age/sex, socio-economic class and household car ownership are good proxies of cake consumption\nacross different parts of the country? No. So we should treat the results as what they are:\nan amusing example of what you can do with spatial microsimulation, not a reliable description\nof the real world. To validate this example we would need to conduct some kind of randomised\nsurvey in the target wards to identify whether cake consumption really does vary in the ways\ndescribed by the model. Most likely we would find that it does not.\n\nThis brings us nicely on to the final point of discussion: how you use spatial microsimulation in\nyour own research. This of course is up to you, but during the process of this course we hope that\nyou have picked up some ideas about best practice in the field. These include:\n\\begin{itemize}\n \\item Ensure, to the extent possible, \\textbf{reproducibility in your method and findings}. This includes, at a minimum,\nclear description of the input data, explanation of the method used and the software needed. As illustrated\nwith the cakeMap example, it is now relatively easy to ensure complete reproducibility even in complex analyses.\nThis will not always be possible due to confidentiality of input data. However, the creation of an example dataset\nand provision of code should always be possible. This is highly recommended as it will greatly help others reproduce\nyour findings (improving the scientific credibility of your research), provide a learning opportunity for yourself and\nothers and increase the probability of other academics citing your work.\n \\item Use spatial microsimulation only when it is the \\textbf{most appropriate tool for the job}. \nThis means that if there are alternatives such as geographically weighted regression analysis\nor analysis of large secondary datasets, these should be considered beforehand. Generating an entirely\nnew individual-level dataset is not to be taken lightly and risks distracting from more grounded research.\nThus it should be seen as an \\emph{addition to} rather than a \\emph{replacement for} more established methods.\n\\item When spatial microsimulation is used, \\textbf{be transparent about its underlying assumptions and limitations}.\nSimply forgetting to include the word `simulated' in the caption of \\cref{fcmap}, for example, could lead\nthe reader to believe it is actual cake consumption that is being described. In the case\nof a cakeMap this may not matter but in areas of public health and the environment the consequences\nof such oversight could be deadly.\n\\end{itemize}\n\nUnderlying each of these points is a wider responsibility: to communicate one's research\nwith clarity and transparency. Too much modelling research is shrouded in a cloud of jargon,\nunstated assumptions and verbose English. If nothing else, this tutorial should\nprovide guidance on how to improve standards in the field and move towards best practice\nfor reproducibility \\citep{Peng2006}.\n\nSpatial microsimulation is a powerful tool.\nLike any powertool, it can acheive very useful results for its user\nbut can also cause great harm if used incorrectly. \nThink of a pneumatic drill: this could be used to build new\npublic infrastructure such as bicycle paths. It could also,\nin clumsy hands, be used to destroy existing infrastructure.\nThe same applies to spatial microsimulation:\nat best it can greatly help out with complex research questions such\nas the distributional impacts of new transport policies \\citep{Lovelace2014-jtg}.\nAt worst, it can waste valuable research time, create misleading results\nand shroud academic research behind an impenetrable wall of jargon.\n\nWe have little doubt that the vast majority of\npeople will prefer the former option. This course has hopefully\nequipped its students with\nthe tools to pursue this lofty aim.\n\n\\section{Glossary} \\label{gloss}\n\\begin{itemize}\n\\item \\textbf{Algorithm}: a series of computer commands which are executed in a well\ndefined order. Algorithms process input data and produce an output.\n\\item \\textbf{Combinatorial optimisation} is an approach to spatial microsimulation that\ngenerates spatial microdata by randomly selecting individuals from a survey dataset \nand measuring the fit between the simulated output and the constraint variables.\nIf the fit improves after any particular change, the change is kept.\n\\citet{Williamson2007} provides a practical user manual for implementing the technique in code.\n\\item \\textbf{Data frame}: a type of object (formally referred to as a class)\nin R, data frames are square tables composed of rows and columns of\ninformation. As with many things in R, the best way to understand data frames is\nto create them and experiment. The following creates a data frame with two\nvariables: name and height:\n\\begin{verbatim}\n data.frame(name = c(\"Robin\", \"Phil\"), height.cm = c(172, 174))\n\\end{verbatim}\nNote that each new variable is entered using the command \\verb c() --- this is\nhow R creates objects with the \\emph{vector} data class, a one dimensional\nmatrix --- and that text data must be entered in quote marks.\n\\item \\textbf{Deterministic reweighting} is an approach to generating spatial\nmicrodata that allocates fractional weights to individuals based on how representative\nthey are of the target area. It differs from combinatorial optimisation approaches in\nthat it requires no random numbers. The most frequently used method of deterministic\nreweighting is IPF.\n\\item \\textbf{For loops} are instructions that tell the computer to run a certain\nset of command repeatedly. \\texttt{for(i in 1:9) {print(i)}}, for example will print\nthe value of i 9 times. The best way to further understand for loops is to try them\nout.\n\\item \\textbf{Iteration}: one instance of a process that is repeated many times until\na predefined end point, often within an \\emph{algorithm}.\n \\item \\textbf{Iterative proportional fitting} (IPF): an iterative process implemented in\nmathematics and algorithms to find the maximum likelihood of cells that are constrained by\nmultiple sets of marginal totals. To make this abstract definition even more confusing, there are\nmultiple terms which refer to the process, including `biproportional fitting' and `matrix raking'.\n% \\citep{Lovelace2014-IPF}\nIn plain English, IPF in the context of spatial microsimulation\ncan be defined as \\emph{a statistical technique for allocating weights to individuals depending\non how representative they are of different zones}. IPF is a type of deterministic reweighting,\nmeaning that random numbers are not needed to generate the result and that the output weights are real (not\ninteger) numbers.\n\\end{itemize}\n\n\\section{Acknowledgements}\nMany thanks to Phil Mike Jones from the University of Sheffield for working through early drafts\nof this document and providing very useful input to the teaching materials overall.\nThank you to Amy O'Neill and Rosie Temple for organising the\n\\href{http://www.ncrm.ac.uk/training/show.php?article=4786}{National Centre for Research Methods (NCRM) course}\n``An Introduction to Spatial Microsimulation using R''\nand dealing with the high demand. Finally, thanks to the open source software movement\noverall, for encouraging best practice in reproducible research and\nmaking powerful tools such as R accessible to anyone, regardless of income, nationality\nor status.\n\n% \\lhead{\\emph{Bibliography}}  % Change the left side page header to\n% \\fancyhead[LO,RE]{\\emph{Bibliography}}\n\\bibliographystyle{plainnat}  % Use the \"unsrtnat\" BibTeX style for\n% \\bibliography{library, lincluded}  % The references (bibliography) information are stored\n\\bibliography{/home/robin/Documents/Microsimulation.bib}  \\label{Bibliography}\n% ,Microsimulation.bib\n% /home/robin/Documents/Microsimulation.bib\n\n\\addtocontents{toc}{\\vspace{2em}}  % Add a gap in the Contents, for aesthetics\n\n%% -----------------------------------------------------------\n\\end{document}  % The End\n",
    "created" : 1410602629355.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1700089986",
    "id" : "BC9A9002",
    "lastKnownWriteTime" : 1410639952,
    "path" : "~/repos/smsim-course/handout.tex",
    "project_path" : "handout.tex",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "tex"
}